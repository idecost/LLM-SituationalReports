{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba6d52b",
   "metadata": {},
   "source": [
    "# SDG-based Answer Organization and Summarization\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "\n",
    "1. **Sorting answers by SDGs**, organizing the files according to their associated Sustainable Development Goals.\n",
    "2. **Updating citations** and generating a **summary for each SDG**, consolidating all questions related to that goal.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **path variables** to specify the input data, output directories, and any other required resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13158d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders\n",
    "answers_folder = \"./Results/Answers/Answers-Subtopics/Dev set/Updated_citations\"\n",
    "sdg_root_folder = \"./Results/Questions/Test questions different prompts/4-Filtred questions with SDGs\"\n",
    "output_folder_answer = \"./Results/Answers/Answers-SDGs\"\n",
    "output_path_summary = \"./Results/Summaries/UniqueSummary-EachSDG\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88fa52",
   "metadata": {},
   "source": [
    "# Sort answers File for SDGs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da0865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping answers_new_cit-answes-Haiti_Gang violence and humanitarian crisis in Haiti-Week 40 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Nigeria_Flooding in Nigeria-Week 37 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-United Kingdom_UK riots-Week 32 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Sudan_Sudan conflict-Week 34 2024-prompt-1.json (output already exists)\n",
      "Processed and saved: ./Results/Answers/Answers-SDGs/answers_new_cit-answes-Jamaica_Hurricane Beryl-Week 28 2024-prompt-1.json\n",
      "Skipping answers_new_cit-answes-Afghanistan_Afghanistan Floods-Week 21 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Bangladesh_Cyclone Remal-Week 21 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Indonesia_Floods and volcanic activity in Indonesia-Week 20 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Ukraine_Ukraine-Week 23 2024-prompt-1.json (output already exists)\n",
      "Processed and saved: ./Results/Answers/Answers-SDGs/answers_new_cit-answes-Israel_Israel_Palestine_confilct-Week 40 2024-prompt-1.json\n",
      "Skipping answers_new_cit-answes-India_LandslideFloods-Week 31 2024-prompt-1.json (output already exists)\n",
      "Skipping answers_new_cit-answes-Israel_Israel-Hamas war-Week 19 2024-prompt-1.json (output already exists)\n",
      "Processed and saved: ./Results/Answers/Answers-SDGs/answers_new_cit-answes-Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024-prompt-1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder_answer, exist_ok=True)\n",
    "\n",
    "# Build a dictionary of all SDG files in all subfolders\n",
    "sdg_files = {}\n",
    "for root, _, files in os.walk(sdg_root_folder):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\"):\n",
    "            sdg_files[f] = os.path.join(root, f)\n",
    "\n",
    "# Iterate over all answer files\n",
    "for answer_file_name in os.listdir(answers_folder):\n",
    "    if not answer_file_name.endswith(\".json\"):\n",
    "        continue\n",
    "    \n",
    "    answer_file_path = os.path.join(answers_folder, answer_file_name)\n",
    "    \n",
    "    # Check if output file already exists\n",
    "    output_file_path = os.path.join(output_folder_answer, answer_file_name)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"Skipping {answer_file_name} (output already exists)\")\n",
    "        continue\n",
    "    \n",
    "    # Construct corresponding SDG file name\n",
    "    sdg_file_name = answer_file_name.replace(\"answers_new_cit-answes-\", \"final_questions-SDGs-final_questions-\")\n",
    "    sdg_file_name_with_ext = sdg_file_name if sdg_file_name.endswith(\".json\") else sdg_file_name + \".json\"\n",
    "    \n",
    "    if sdg_file_name_with_ext not in sdg_files:\n",
    "        print(f\"SDG file not found for {answer_file_name}\")\n",
    "        continue\n",
    "    \n",
    "    sdg_file_path = sdg_files[sdg_file_name_with_ext]\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(answer_file_path, 'r', encoding='utf-8') as f:\n",
    "        answers_data = json.load(f)\n",
    "    \n",
    "    with open(sdg_file_path, 'r', encoding='utf-8') as f:\n",
    "        sdg_data = json.load(f)\n",
    "    \n",
    "    # Flatten SDG data into question -> SDG list\n",
    "    question_to_sdgs = {}\n",
    "    for cluster_id, questions in sdg_data.items():\n",
    "        for q in questions:\n",
    "            question_to_sdgs[q[\"question\"]] = q[\"sdg_scores\"]\n",
    "    \n",
    "    # Prepare SDG grouped output (only create keys for SDGs with answers)\n",
    "    sdg_grouped = {}\n",
    "    \n",
    "    for item in answers_data:\n",
    "        question = item.get(\"question\")\n",
    "        if question not in question_to_sdgs:\n",
    "            continue\n",
    "        sdg_scores = question_to_sdgs[question]\n",
    "        # For each SDG index with score 1, add this answer\n",
    "        for sdg_index, score in enumerate(sdg_scores):\n",
    "            if score:\n",
    "                sdg_key = f\"sdg-{sdg_index + 1}\"  # <-- index+1 for 1-based SDG numbering\n",
    "                if sdg_key not in sdg_grouped:\n",
    "                    sdg_grouped[sdg_key] = []\n",
    "                sdg_grouped[sdg_key].append(item)\n",
    "    \n",
    "    # Save output\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sdg_grouped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed and saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbde4",
   "metadata": {},
   "source": [
    "# Create summary for each SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Skipping file: Afghanistan_Afghanistan Floods-Week 21 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: Bangladesh_Cyclone Remal-Week 21 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: Haiti_Gang violence and humanitarian crisis in Haiti-Week 40 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: India_LandslideFloods-Week 31 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: Indonesia_Floods and volcanic activity in Indonesia-Week 20 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: Israel_Israel-Hamas war-Week 19 2024 (output already exists) ---\n",
      "\n",
      "--- Processing file: Israel_Israel_Palestine_confilct-Week 40 2024 ---\n",
      "âœ… Summarized sdg-3 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-16 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-2 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-9 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-15 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-1 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-6 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-11 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-10 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-4 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-5 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "âœ… Summarized sdg-17 for Israel_Israel_Palestine_confilct-Week 40 2024\n",
      "ðŸ’¾ Saved summary for Israel_Israel_Palestine_confilct-Week 40 2024 to ./Results/Summaries/UniqueSummary-EachSDG/summary_Israel_Israel_Palestine_confilct-Week 40 2024.json\n",
      "\n",
      "--- Processing file: Jamaica_Hurricane Beryl-Week 28 2024 ---\n",
      "âœ… Summarized sdg-1 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-2 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-8 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-15 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-13 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-9 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-11 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-10 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-3 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-6 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-17 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-16 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-7 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "âœ… Summarized sdg-5 for Jamaica_Hurricane Beryl-Week 28 2024\n",
      "ðŸ’¾ Saved summary for Jamaica_Hurricane Beryl-Week 28 2024 to ./Results/Summaries/UniqueSummary-EachSDG/summary_Jamaica_Hurricane Beryl-Week 28 2024.json\n",
      "\n",
      "--- Skipping file: Nigeria_Flooding in Nigeria-Week 37 2024 (output already exists) ---\n",
      "\n",
      "--- Processing file: Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024 ---\n",
      "âœ… Summarized sdg-1 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-8 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-10 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-2 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-3 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-6 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-11 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-13 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-16 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-15 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "âœ… Summarized sdg-9 for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024\n",
      "ðŸ’¾ Saved summary for Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024 to ./Results/Summaries/UniqueSummary-EachSDG/summary_Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024.json\n",
      "\n",
      "--- Skipping file: Sudan_Sudan conflict-Week 34 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: Ukraine_Ukraine-Week 23 2024 (output already exists) ---\n",
      "\n",
      "--- Skipping file: United Kingdom_UK riots-Week 32 2024 (output already exists) ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Input and output paths\n",
    "\n",
    "\n",
    "# OpenAI client\n",
    "client = OpenAI(api_key=\" \")\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"\n",
    "Your task is to integrate the following pieces of text into a single, cohesive, and flowing narrative. The goal is to present as much of the original information as possible, not to summarize it briefly.\n",
    "\n",
    "The text contains information with citations, formatted as `[number]`. It is crucial that you adhere to the following rules:\n",
    "\n",
    "1. **Integrate all key information**: Combine sentences and ideas from the input to form a comprehensive and coherent text. Aim to include a good portion, if not all, of the provided details.\n",
    "2. **Maintain original citations**: Every piece of information you include in the integrated text must retain its original citation(s).\n",
    "3. **Handle citations when combining**: If you rephrase or combine sentences containing information from multiple sources, ensure that *all* relevant original citation numbers for that combined information are included at the end of the new sentence. For example, if a new sentence merges details from original sentences cited `[1]` and `[5]`, the new sentence should be followed by `[1][5]`.\n",
    "4. **Ensure logical flow**: Arrange the information in a way that creates a natural and readable progression of ideas, even if it means reordering content from the original input.\n",
    "5. **Avoid external knowledge**: Your output must be based *solely* on the information provided in the input text. Do not introduce any outside facts or personal opinions.\n",
    "\n",
    "Here is the text to integrate:\n",
    "{text_to_integrate}\n",
    "\"\"\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_path_summary, exist_ok=True)\n",
    "\n",
    "# Collect all input files\n",
    "try:\n",
    "    json_files = np.sort([f for f in os.listdir(output_folder_answer) if f.endswith('.json')])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{output_folder_answer}' was not found.\")\n",
    "    json_files = []\n",
    "\n",
    "# Process each SDG JSON file\n",
    "for file in json_files:\n",
    "    file_name = file.replace('answers_new_cit-answes-', '').replace('-prompt-1.json', '')\n",
    "    \n",
    "    # Check if output file already exists\n",
    "    output_filename = os.path.join(output_path_summary, f\"summary_{file_name}.json\")\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"--- Skipping file: {file_name} (output already exists) ---\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"--- Processing file: {file_name} ---\")\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(output_folder_answer, file), 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from file: {file}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Group answers by SDG key\n",
    "    sdg_groups = {}\n",
    "    for sdg_key, answers in json_data.items():\n",
    "        sdg_groups[sdg_key] = [a for a in answers if a]  # Keep full item for context tracking\n",
    "\n",
    "    # Process each SDG group\n",
    "    modified_answers = {}\n",
    "    used_contexts = {}\n",
    "\n",
    "    for sdg_key, answers_in_group in sdg_groups.items():\n",
    "        modified_answers[sdg_key] = []\n",
    "        used_contexts[sdg_key] = {}\n",
    "\n",
    "        for i, item in enumerate(answers_in_group):\n",
    "            original_answer = item.get(\"updated_retrieved_answer\", \"\")\n",
    "            offset = i * 10\n",
    "            modified_answer = original_answer\n",
    "\n",
    "            citations_found = set(re.findall(r'\\[(\\d+)\\]', original_answer))\n",
    "            sorted_citations = sorted([int(c) for c in citations_found], reverse=True)\n",
    "\n",
    "            for old_citation_num in sorted_citations:\n",
    "                new_citation_num = old_citation_num + offset\n",
    "                modified_answer = re.sub(\n",
    "                    r'\\[{}\\]'.format(re.escape(str(old_citation_num))),\n",
    "                    '[{}]'.format(new_citation_num),\n",
    "                    modified_answer\n",
    "                )\n",
    "\n",
    "                # Track context if available\n",
    "                if \"new_citations\" in item and old_citation_num in item[\"new_citations\"]:\n",
    "                    idx = item[\"new_citations\"].index(old_citation_num)\n",
    "                    ctx = item.get(\"new_used_contexts\", [])[idx]\n",
    "                    used_contexts[sdg_key][new_citation_num] = ctx\n",
    "\n",
    "            modified_answers[sdg_key].append(modified_answer)\n",
    "\n",
    "    # Generate summaries for each SDG group\n",
    "    final_output = {}\n",
    "\n",
    "    for sdg_key, answers in modified_answers.items():\n",
    "        if not answers:\n",
    "            continue\n",
    "        text_to_summarize = \"\\n\\n\".join(answers)\n",
    "        custom_prompt = prompt_template.format(text_to_integrate=text_to_summarize)\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in combining text.\"},\n",
    "                    {\"role\": \"user\", \"content\": custom_prompt}\n",
    "                ],\n",
    "                temperature=0.0\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            final_output[sdg_key] = {\n",
    "                \"summary\": summary,\n",
    "                \"used_contexts\": used_contexts[sdg_key]\n",
    "            }\n",
    "            print(f\"âœ… Summarized {sdg_key} for {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error: Could not summarize due to {e}\"\n",
    "            final_output[sdg_key] = {\n",
    "                \"summary\": error_message,\n",
    "                \"used_contexts\": used_contexts[sdg_key]\n",
    "            }\n",
    "            print(f\"âŒ Error summarizing {sdg_key} for {file_name}: {e}\")\n",
    "\n",
    "    # Save the summarized answers to a new JSON file\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Saved summary for {file_name} to {output_filename}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not write to file {output_filename}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4130816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation per Cluster Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we continue from the **clusters obtained in the previous notebook**.  \n",
    "\n",
    "For each cluster, we **generate relevant questions** using a large language model (LLM).  \n",
    "\n",
    "We then apply **nucleus sampling** to produce a diverse set of questions while maintaining coherence and relevance.  \n",
    "\n",
    "Next, we perform **deduplication** to remove redundant or overlapping questions, ensuring a concise and high-quality question set for each cluster.  \n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **variables** and **path definitions** to specify the input clusters, model configuration, sampling parameters, and output directories used throughout the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_number = 1\n",
    "file_name = \"Israel_Israel-Hamas war-Week 19 2024\"\n",
    "event = \"Isreal and Palestine conflict \"\n",
    "country = 'Israel'\n",
    "input_file = f\"Results/Cluster/Clusters+Headline /clusters-{file_name}.json\"\n",
    "#input_file = \"/Users/decostanzi/Desktop/Project-ISI/SmartBook/SmartBook-Reports/Pipeline_DatasetDerya/Results/Clusters+Headline /clusters-{input_file}.json\"\n",
    "output_dir = \"Results/Questions/Test questions different prompts/1-Questions generated/Dev set\"\n",
    "#output_dir_deduplicated = \"Results/Questions/Test questions different prompts/2-Questions Deduplicated across clusters\"\n",
    "\n",
    "\n",
    "generation_model_path = \"/Users/decostanzi/Desktop/Project-ISI/SmartBook/SmartBook/question_generation/models/t5-base-canard-mode\"\n",
    "duplicate_question_model_path = \"/Users/decostanzi/Desktop/Project-ISI/SmartBook/SmartBook/question_generation/models/cross-encoder\"  \n",
    "expand_question_model_path = \"/Users/decostanzi/Desktop/Project-ISI/SmartBook/SmartBook/question_generation/models/quora-roberta-base-model\"\n",
    "duplicate_treshold = 0.7\n",
    "output_name = f\"questions-{file_name}-prompt-{prompt_number}.json\"\n",
    "\n",
    "openaikey = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "#from openai.error import RateLimitError\n",
    "import backoff\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
    "from copy import deepcopy\n",
    "from sentence_transformers import CrossEncoder\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openaikey\n",
    "client = OpenAI()\n",
    "\n",
    "def get_questions_from_openai(prompt):\n",
    "    chatgpt_output = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    questions = chatgpt_output.choices[0].message.content.strip()\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(headline, prompt_number, event, country):\n",
    "    prompt1 = f'''\n",
    "You are an expert in developing strategic and tactical questions to analyze and address humanitarian situations, based exclusively on the provided data.\n",
    "Your task is to generate clear, specific, and insightful questions tailored for a humanitarian situational report.\n",
    "\n",
    "Input:\n",
    "You will be provided with:\n",
    "1.  A set of paragraphs extracted from humanitarian documents.\n",
    "2.  A headline summarizing the cluster.\n",
    "3.  The specific event: {event}.\n",
    "4.  The relevant country: {country}.\n",
    "\n",
    "Instructions for Generating Questions:\n",
    "\n",
    "1.  Data-Driven: Each question must rely *solely* on the information present in the provided text. Do not introduce external knowledge or ask about information not contained in the text.\n",
    "2.  Relevance: Ensure every question is directly relevant to the content of the provided paragraphs, the specified {event}, and the {country}. The questions should aim to capture key aspects of the humanitarian situation described.\n",
    "3.  Precision: Questions must be well-defined, focused, and unambiguous.\n",
    "4.  Action-Oriented: Frame questions to elicit actionable insights that can support humanitarian decision-making and response, based *only* on the data provided.\n",
    "5.  Explicit Acronyms: If an acronym is used in a question (e.g., WHO), it must be explicitly defined within that question (e.g., \"What actions has the World Health Organization (WHO) taken...\"), assuming the definition is available or inferable from the provided text. If the text uses an acronym without defining it, the question may also use it if it's central to the text, but define it if possible from context.\n",
    "6.  Neutral and Non-Political: Questions must maintain a strictly neutral tone. Avoid any political content, expressions of personal or subjective opinions, or leading questions.\n",
    "\n",
    "Output Format Requirements:\n",
    "\n",
    "* Questions Only: Your output must consist *only* of the generated questions. Do not include any introductory text, preambles, explanations, or concluding remarks (e.g., do NOT start with \"Here are some questions:\" or similar phrases).\n",
    "* No Categorization or Prefixes: Do not add any category titles, labels (e.g., \"Humanitarian Needs:\"), or any other descriptive text before individual questions.\n",
    "* No Formatting: Generate questions in plain text. Do not use any formatting such as bolding, italics, or asterisks.\n",
    "* Listing: Present the questions as a simple list. Each question should ideally start on a new line. Numbering (e.g., 1., 2., 3.) is acceptable and preferred if multiple questions are generated.\n",
    "\n",
    "\n",
    "Headline: {headline}  \n",
    "\n",
    "Content:\n",
    "'''\n",
    "    prompt2 = f'''\n",
    "You are an expert in creating strategic and tactical questions to analyze and address humanitarian situations based solely on the provided data.  \n",
    "Your task is to generate clear, specific, and insightful questions tailored for a humanitarian situational report.  \n",
    "\n",
    "Requirements for the questions:  \n",
    "1. **Data-Driven:** The questions must rely solely on the information in the provided text, without requiring any external knowledge.  \n",
    "2. **Relevance:** Ensure the questions are directly relevant to the content and capture key aspects of the text. The questions should be relevant to the event  {event} and related to the country {country}. \n",
    "3. **Precision:** Questions should be well-defined and focused, avoiding ambiguity.  \n",
    "4. **Action-Oriented:** Aim to elicit actionable insights that can support humanitarian decision-making. \n",
    "5. **Explicit Acronyms:** If an acronym is used, it must be explicitly defined within the question.\n",
    "\n",
    "**Chain of Thought:**\n",
    "Understand the Context: Read the headline and content carefully to grasp the key humanitarian issues.\n",
    "Highlight Important Details: Identify critical facts such as needs, risks, or affected groups.\n",
    "Ask Clarifying Questions: Think of questions that provide a deeper understanding without needing outside information.\n",
    "Ensure Answerability: Make sure each question can be answered directly from the content provided.\n",
    "\n",
    "You will receive a set of paragraphs extracted from humanitarian documents along with a headline summarizing the cluster.  \n",
    "\n",
    "Headline: {headline}  \n",
    "\n",
    "Content:\n",
    "'''\n",
    "\n",
    "    prompt3 = f'''\n",
    "    You are an expert in creating strategic and tactical questions to analyze and address humanitarian situations based solely on the provided data.  \n",
    "Your task is to generate clear, specific, and insightful questions tailored for a humanitarian situational report.  \n",
    "\n",
    "Requirements for the questions:  \n",
    "1. **Data-Driven:** The questions must rely solely on the information in the provided text, without requiring any external knowledge.  \n",
    "2. **Relevance:** Ensure the questions are directly relevant to the content and capture key aspects of the text. The questions should be relevant to the event  {event} and related to the country {country}. \n",
    "3. **Precision:** Questions should be well-defined and focused, avoiding ambiguity.  \n",
    "4. **Action-Oriented:** Aim to elicit actionable insights that can support humanitarian decision-making. \n",
    "5. **Explicit Acronyms:** If an acronym is used, it must be explicitly defined within the question.\n",
    "\n",
    "**Chain of Thought:**\n",
    "Understand the Context: Read the headline and content carefully to grasp the key humanitarian issues.\n",
    "Highlight Important Details: Identify critical facts such as needs, risks, or affected groups.\n",
    "Ask Clarifying Questions: Think of questions that provide a deeper understanding without needing outside information.\n",
    "Ensure Answerability: Make sure each question can be answered directly from the content provided.\n",
    "    \n",
    "**Examples of good questions for a situational report:**  \n",
    "- What are the possible motives for sabotage of the Nord Stream gas pipelines?  \n",
    "- What is the strategic importance of the new U.S. Embassy in Tonga's capital, Nuku'alofa?\n",
    "- What are the patterns emerging from the frequency and magnitude of the aftershocks following the main quake in southern Turkey?  \n",
    "- What measures has the IAEA taken to ensure the safety of Ukraine's nuclear power plants?\n",
    "The previous are just examples of the type of questions I expect. The model should generate new questions based on the provided text, not repeat these.\n",
    "\n",
    "You will receive a set of paragraphs extracted from humanitarian documents along with a headline summarizing the cluster.  \n",
    "\n",
    "Headline: {headline}  \n",
    "Content:\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    prompts = [prompt1, prompt2, prompt3]\n",
    "    \n",
    "    # Ensure valid prompt selection\n",
    "    if 1 <= prompt_number <= len(prompts):\n",
    "        return prompts[prompt_number - 1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt number. Please select 1, 2, or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_questions(data, prompt_number , event, country, generation_model_path=None):\n",
    "    \n",
    "    for cluster_index in tqdm(data):\n",
    "        item = data[cluster_index]\n",
    "        articles = item[\"cluster_articles\"]\n",
    "        headline = item[\"cluster_headline\"]\n",
    "            \n",
    "\n",
    "        input_LLM = get_prompt(headline, prompt_number=prompt_number, event=event, country=country)\n",
    "        \n",
    "        \n",
    "\n",
    "        non_null_texts = [article[\"text\"] for article in articles if article[\"text\"].strip()]\n",
    "        for index, article in enumerate(non_null_texts):\n",
    "            text = \" \".join(article.split(\"\\n\")[0:]).strip()\n",
    "            input_LLM += f\"{index + 1}) {text}\\n\"\n",
    "\n",
    "        # Chain-of-thought approach:\n",
    "        \n",
    "\n",
    "        print(input_LLM)\n",
    "\n",
    "        data[cluster_index][\"questions\"] = []\n",
    "\n",
    "        for _ in range(3):\n",
    "            if True:\n",
    "                questions = get_questions_from_openai(input_LLM)\n",
    "            else:\n",
    "                \n",
    "                questions = None  # Placeholder\n",
    "\n",
    "            data[cluster_index][\"questions\"].append(questions)\n",
    "\n",
    "        data[cluster_index][\"article_titles\"] = [\n",
    "            article[\"text\"].split(\"\\n\")[0].strip() for article in item[\"cluster_articles\"]\n",
    "        ]\n",
    "        data[cluster_index][\"question_sets\"] = [\n",
    "            question.split(\"\\n\") for question in data[cluster_index][\"questions\"]\n",
    "        ]\n",
    "\n",
    "        # Clean up unnecessary fields\n",
    "        del data[cluster_index][\"cluster_articles\"]\n",
    "        del data[cluster_index][\"questions\"]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_questions(data, expand_question_model_path):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"castorini/t5-base-canard\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"castorini/t5-base-canard\")\n",
    "    #model.to(\"cuda\")\n",
    "    print(\"Model for expanded questions loaded correctly\")\n",
    "    \n",
    "    for cluster_index in tqdm(data):\n",
    "        cluster = data[cluster_index]\n",
    "        title = cluster[\"cluster_headline\"]\n",
    "        data[cluster_index][\"expanded_questions\"] = list()\n",
    "        for question_set in cluster[\"question_sets\"]:\n",
    "            expand_questions = list()\n",
    "            question_base = \" \".join(question_set[0].split()[1:])\n",
    "            expand_questions.append(question_base)\n",
    "\n",
    "            for question in question_set[1:]:\n",
    "                context = title + \" ||| \" + question_base + \" ||| \" + \" \".join(question.split()[1:])\n",
    "                #input_ids = tokenizer(context,return_tensors=\"pt\").input_ids.cuda()\n",
    "                input_ids = tokenizer(context,return_tensors=\"pt\").input_ids\n",
    "                outputs = model.generate(input_ids, max_length=100)\n",
    "                question_new = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                expand_questions.append(question_new)\n",
    "            data[cluster_index][\"expanded_questions\"].append(deepcopy(expand_questions))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_questions(data):\n",
    "    for cluster_index in tqdm(data):\n",
    "        data[cluster_index][\"filtered_questions\"] = list()\n",
    "        cluster = data[cluster_index]\n",
    "        for question_set in cluster[\"question_sets\"]:\n",
    "            filtered_questions = list()\n",
    "            for question in question_set:\n",
    "                question = question.strip()\n",
    "                if question and question[-1] == \"?\":\n",
    "                    filtered_questions.append(question)\n",
    "            if len(filtered_questions) >= 1:\n",
    "                data[cluster_index][\"filtered_questions\"].append(deepcopy(filtered_questions))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data, duplicate_question_model_path, threshold):\n",
    "    model = CrossEncoder(\"cross-encoder/quora-roberta-base\", device='cpu')\n",
    "    for cluster_index in tqdm(data):\n",
    "        cluster = data[cluster_index]\n",
    "        print(\"Title: \", cluster[\"cluster_headline\"])\n",
    "        print(\"\\n\")\n",
    "        all_questions = list()\n",
    "        for s in cluster[\"filtered_questions\"]:\n",
    "            all_questions.extend(s)\n",
    "        qset = [all_questions[0]]\n",
    "        for question in all_questions[1:]:\n",
    "            q_list = [(q, question) for q in qset]\n",
    "            scores = model.predict(q_list)\n",
    "            max_si = np.argmax(scores)\n",
    "            if np.max(scores) < threshold:\n",
    "                qset.append(question)\n",
    "        data[cluster_index][\"unique_questions\"] = deepcopy(qset)\n",
    "        qset = qset[1:]\n",
    "        random.shuffle(qset)\n",
    "        data[cluster_index][\"picked_questions\"] = list()\n",
    "        data[cluster_index][\"picked_questions\"].append(data[cluster_index][\"unique_questions\"][0])\n",
    "        data[cluster_index][\"picked_questions\"].extend(qset[:5])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_across_clusters(data, duplicate_question_model_path, threshold):\n",
    "    \"\"\"\n",
    "    Removes duplicate questions across clusters and equalizes similar ones.\n",
    "    \n",
    "    Parameters:\n",
    "        data (dict): The data dictionary containing clusters.\n",
    "        duplicate_question_model_path (str): Path to the pre-trained model.\n",
    "        threshold (float): Similarity threshold to consider two questions as duplicates.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated data dictionary with duplicates removed across clusters.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = CrossEncoder(duplicate_question_model_path, device='cpu')\n",
    "    \n",
    "    # Extract all picked questions with their cluster indices\n",
    "    cluster_questions = [\n",
    "        (cluster_index, question)\n",
    "        for cluster_index in data\n",
    "        for question in data[cluster_index][\"picked_questions\"]\n",
    "    ]\n",
    "    \n",
    "    # Create a unique mapping of questions\n",
    "    unique_questions = {}\n",
    "    \n",
    "    for i, (cluster_index, question) in enumerate(tqdm(cluster_questions, desc=\"Processing clusters\")):\n",
    "        duplicate_found = False\n",
    "        for unique_question in unique_questions:\n",
    "            # Compare current question with all unique questions\n",
    "            score = model.predict([(unique_question, question)])[0]\n",
    "            if score >= threshold:\n",
    "                # If a duplicate is found, map the current question to the unique one\n",
    "                unique_questions[unique_question].append((cluster_index, question))\n",
    "                duplicate_found = True\n",
    "                break\n",
    "        \n",
    "        if not duplicate_found:\n",
    "            # If no duplicate is found, add the question as a new unique one\n",
    "            unique_questions[question] = [(cluster_index, question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_across_clusters(data, duplicate_question_model_path, threshold):\n",
    "    \"\"\"\n",
    "    Tracks duplicate questions across clusters while keeping all questions intact.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data dictionary containing clusters.\n",
    "        duplicate_question_model_path (str): Path to the model for detecting duplicate questions.\n",
    "        threshold (float): Threshold for considering questions as duplicates.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated data dictionary with duplicates tracked across clusters.\n",
    "    \"\"\"\n",
    "    model = CrossEncoder(duplicate_question_model_path, device='cpu')\n",
    "    global_question_map = {}  # To map questions to their original cluster\n",
    "\n",
    "    for cluster_index in tqdm(data):\n",
    "        cluster = data[cluster_index]\n",
    "        print(\"Processing cluster:\", cluster[\"cluster_headline\"])\n",
    "        \n",
    "        # Initialize duplicate tracking for the current cluster\n",
    "        cluster[\"duplicate_questions\"] = []\n",
    "        \n",
    "        for question in cluster[\"picked_questions\"]:\n",
    "            is_duplicate = False\n",
    "            for seen_question, seen_cluster in global_question_map.items():\n",
    "                score = model.predict([(seen_question, question)])[0]\n",
    "                if score >= threshold:\n",
    "                    is_duplicate = True\n",
    "                    # Add duplicate info (question, first seen cluster) to current cluster\n",
    "                    cluster[\"duplicate_questions\"].append((question, seen_cluster))\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                # Add the question to the global map if it's not a duplicate\n",
    "                global_question_map[question] = cluster_index\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(input_file, \"rb\") as f:\n",
    "        headline_data = json.load(f)\n",
    "\n",
    "# Process the data\n",
    "\n",
    "questions = generate_questions(headline_data, prompt_number=prompt_number, event=event, country=country)\n",
    "expanded_questions = expand_questions(questions, expand_question_model_path)\n",
    "print(\"Questions expanded correctly\")\n",
    "\n",
    "filtered_questions = filter_questions(expanded_questions)\n",
    "print(\"Questions filtered correctly\")\n",
    "\n",
    "final_questions = remove_duplicates(filtered_questions, duplicate_question_model_path, duplicate_treshold)\n",
    "\n",
    "output_file = os.path.join(output_dir, output_name)\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(final_questions, f, indent=4)\n",
    "\n",
    "print(f\"Final questions saved to {output_file}\")\n",
    "\n",
    "\n",
    "# deduplicated_data = duplicates_across_clusters(final_questions, duplicate_question_model_path, duplicate_treshold)\n",
    "# output_file_deduplicated = os.path.join(output_dir_deduplicated, output_name)\n",
    "# with open(output_file_deduplicated, \"w\") as f:\n",
    "#     json.dump(deduplicated_data, f, indent=4)\n",
    "    \n",
    "# print(f\"Deduplicated questions saved to {output_file_deduplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

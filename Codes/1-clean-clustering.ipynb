{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering and Summarization Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we continue from the **JSON file extracted in the first notebook**.  \n",
    "\n",
    "We begin by **cleaning the text**, removing noise and irrelevant content. Once preprocessed, the text is **split into paragraphs** to facilitate more granular analysis.  \n",
    "\n",
    "We then **embed the paragraphs**, apply **UMAP** for dimensionality reduction, and use **HDBSCAN** to **cluster similar claims** based on their semantic similarity.  \n",
    "\n",
    "To **evaluate the quality of the resulting clusters**, we define two metrics.  \n",
    "\n",
    "Finally, for each cluster of documents, we **prompt a large language model (LLM)** to generate a **representative title** that summarizes the content of the cluster.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **variables** and **path definitions** to specify the input data, model configuration, and output directories used throughout the workflow.\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "openai_key = \"\"\n",
    "\n",
    "file_name = \"Afghanistan_Afghanistan Floods-Week 21 2024\"\n",
    "\n",
    "# Input folders\n",
    "folder_sources_metadata_path = Path(\"Results/Sources/SourcesCountryEvent-Metadata/Dev set/\")\n",
    "folder_sources_path = Path(\"Results/Sources/SourcesCountryEvent/Dev set/\")\n",
    "\n",
    "\n",
    "# Output folder\n",
    "folder_metadata_paragraphs = Path(\"Results/paragraphs_metadata/\")\n",
    "folder_cluster_path = Path(\"Results/Cluster/Clusters\")\n",
    "folder_cluster_headline_path = Path(\"Results/Cluster/Clusters+Headline \")\n",
    "\n",
    "\n",
    "# Full file paths (adding .json)\n",
    "sources_path = folder_sources_path / f\"{file_name}.json\"\n",
    "sources_metadata_path = folder_sources_metadata_path / f\"sources-metadata-{file_name}.json\"\n",
    "\n",
    "# Read JSON files\n",
    "sources_text = json.loads(sources_path.read_text(encoding='utf-8'))\n",
    "sources_metadata = json.loads(sources_metadata_path.read_text(encoding='utf-8-sig'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.replace('\\\\xe2\\\\x80\\\\x9c', '\"')\n",
    "    text = text.replace('\\\\xe2\\\\x80\\\\x9d', '\"')\n",
    "    text = text.replace('\\\\xe2\\\\x80\\\\x98', \"'\")\n",
    "    text = text.replace('\\\\xe2\\\\x80\\\\x99', \"'\")\n",
    "    text = text.replace('\\u201c', '\"')\n",
    "    text = text.replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\\\xc2\\\\xa0', \" \")\n",
    "    text = text.replace('\\\\\"', '\\\"')\n",
    "    text = text.replace(\"\\\\'\", \"\\'\")\n",
    "    text = text.replace(\"\\\\xe2\\\\x80\\\\x94\", \"-\")\n",
    "    text = text.replace(\"\\\\xe2\\\\x80\\\\x93\", \"-\")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\uf0b7\", \"\")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:\\'\\\"-]', '', text)\n",
    "    text = text.replace(\"\\xa0\", \"\")\n",
    "    text = re.sub(r\"\\.{4,}\", '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text )\n",
    "    \n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes a text into sentences and remove the short sentecences.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Remove sentences that are too short (less than 4 words)\n",
    "    \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def remove_bad_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [s for s in sentences if len(s.split()) >= 4 ]\n",
    "    \n",
    "    filtered_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check if any word in the sentence is longer than 35 characters\n",
    "        if any(len(word) > 35 for word in sentence.split()):\n",
    "            continue\n",
    "        \n",
    "        # Check if the sentence contains three or more periods\n",
    "        if sentence.count('.') >= 3:\n",
    "            continue\n",
    "        \n",
    "        # If both conditions are passed, add to the filtered list\n",
    "        filtered_sentences.append(sentence)\n",
    "    \n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in paragraphs and add metadata to the paragraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "folder_metadata_paragraphs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for source_file in folder_sources_path.iterdir():\n",
    "    # Skip non-JSON files and metadata files\n",
    "    if not source_file.name.endswith(\".json\") or source_file.name.startswith(\"sources-metadata\"):\n",
    "        continue\n",
    "\n",
    "    base_name = source_file.stem  # file name without .json\n",
    "    metadata_file = folder_sources_metadata_path / f\"sources-metadata-{base_name}.json\"\n",
    "\n",
    "    # Skip if metadata file doesn't exist\n",
    "    if not metadata_file.exists():\n",
    "        print(f\"Metadata file missing for {source_file.name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load source and metadata files\n",
    "    with source_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        sources_text = json.load(f)\n",
    "\n",
    "    with metadata_file.open(\"r\", encoding=\"utf-8-sig\") as f:\n",
    "        sources_metadata = json.load(f)\n",
    "\n",
    "    full_paragraphs_metadata = []\n",
    "\n",
    "    for key, data in sources_metadata.items():\n",
    "        title = data['title']\n",
    "        content = data['content']\n",
    "        url = data['url']\n",
    "\n",
    "        sentences = get_sentences(content)\n",
    "        sentences = remove_bad_sentences(content)\n",
    "\n",
    "        # Split sentences into paragraphs of 4 sentences each\n",
    "        paragraphs = [\" \".join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            full_paragraphs_metadata.append({\"title\": title, \"url\": url, \"paragraph\": paragraph})\n",
    "\n",
    "    # Save metadata to output folder\n",
    "    output_file = folder_metadata_paragraphs / f\"metadata-{base_name}.json\"\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_paragraphs_metadata, f, indent=2)\n",
    "\n",
    "    print(f\"Saved metadata for {source_file.name} to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed(model, sentences):\n",
    "    \"\"\"\n",
    "    wrapper function for generating embeddings\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "def generate_clusters(embeddings,\n",
    "                      n_neighbors,\n",
    "                      min_cluster_size,\n",
    "                      min_samples, \n",
    "                      cluster_selection_epsilon, \n",
    "                      random_state = None):\n",
    "    \"\"\"\n",
    "    Generate HDBSCAN cluster object after reducing embedding dimensionality with UMAP\n",
    "    \"\"\"\n",
    "    \n",
    "    umap_embeddings = (umap.UMAP(n_neighbors=n_neighbors, \n",
    "                                n_components= 10, \n",
    "                                metric='cosine', \n",
    "                                random_state=random_state)\n",
    "                            .fit_transform(embeddings))\n",
    "\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, \n",
    "                               min_samples = min_samples,\n",
    "                               cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                               metric='euclidean', \n",
    "                               cluster_selection_method='eom', gen_min_span_tree=True).fit(umap_embeddings)\n",
    "    \n",
    "    \n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_clusters(sentences, clusters):\n",
    "    cluster_labels = clusters.labels_\n",
    "    unique_labels = set(cluster_labels)\n",
    "    \n",
    "    cluster_dict = {}\n",
    "    for label in unique_labels:\n",
    "        cluster_dict[label] = []\n",
    "\n",
    "    for sentence, label in zip(sentences, cluster_labels):\n",
    "        cluster_dict[label].append(sentence)\n",
    "    \n",
    "    result_dict = {}\n",
    "    for label, cluster_sentences in cluster_dict.items():\n",
    "        result_dict[label] = cluster_sentences\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_parameters(nsentences):\n",
    "\n",
    "    \n",
    "   \n",
    "    n_neighbors = range(3, 30, 3)  \n",
    "    min_cluster_size = range(3, 30, 3) \n",
    "    min_samples = range(2, 20, 2)  \n",
    "    cluster_selection_epsilon = [ 0.05, 0.1, 0.15, 0.2 ]  \n",
    "    return {\n",
    "        \"n_neighbors\": n_neighbors,\n",
    "        \"min_cluster_size\": min_cluster_size,\n",
    "        \"min_samples\": min_samples,\n",
    "        \"cluster_selection_epsilon\": cluster_selection_epsilon,\n",
    "        \"random_state\": 101\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dbcv_score(clusters, prob_threshold = 0.05):\n",
    "    \"\"\"\n",
    "    Returns the label count and cost of a given cluster supplied from running hdbscan\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = clusters.labels_\n",
    "    label_count = len(np.unique(cluster_labels))\n",
    "    # total_num = len(clusters.labels_)\n",
    "    # cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)/total_num)\n",
    "    \n",
    "    cost = clusters.relative_validity_\n",
    "    return label_count, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "def evaluate_cluster_LLM(cluster, openaikey, temperature=0.1):\n",
    "    prompt =f\"\"\"\n",
    "    Evaluate the coherence and homogeneity of the following cluster of humanitarian documents. \n",
    "    These documents need to be clustered in a way that supports the effective generation of questions, ensuring that the cluster is logically consistent and semantically relevant.\n",
    "\n",
    "    **Evaluation Criteria**:\n",
    "    - **Coherence**: How logically connected are the items in the cluster? Do the items form a consistent and sensible narrative when taken together?\n",
    "    - **Homogeneity**: How similar or uniform are the items in the cluster in terms of topic, content, and style? Are there any outliers that introduce dissimilarity?\n",
    "\n",
    "    **Evaluation Process**:\n",
    "    1. **Evaluate Coherence**:\n",
    "       Assess how logically connected and internally consistent the items are. Do the items follow a clear and consistent line of thought or theme? Is there any inconsistency or contradiction within the cluster?\n",
    "\n",
    "    2. **Evaluate Homogeneity**:\n",
    "       Determine the degree of similarity or uniformity within the cluster. Are the items highly similar to each other in terms of subject matter, style, or content? If there are significant differences, how do they affect the overall homogeneity?\n",
    "\n",
    "    3. **Coherence and Homogeneity Scores**:\n",
    "       Based on your evaluation of both coherence and homogeneity, assign a score for each aspect on a scale from 0 to 1:\n",
    "       - **0** means poor coherence/homogeneity (e.g., the items are disconnected or highly dissimilar).\n",
    "       - **1** means excellent coherence/homogeneity (e.g., the items are logically connected and highly similar).\n",
    "       - A score in between reflects varying degrees of coherence/homogeneity.\n",
    "\n",
    "    Cluster:\n",
    "    {cluster}\n",
    "\n",
    "    Please compute the following:\n",
    "    \n",
    "    Coherence score (0 to 1) \n",
    "    Homogeneity score (0 to 1)\n",
    "    and return just the mean of the scores, not other words. \n",
    "    \n",
    "    \"\"\"\n",
    "    client = openai.OpenAI(api_key= openaikey)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract the model's score\n",
    "    score_text = response.choices[0].message.content\n",
    "    \n",
    "    return score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_clusters_LLM(sentences, clusters):\n",
    "    dict_clsuters = display_clusters(sentences, clusters)\n",
    "    scores = np.zeros(len(dict_clsuters))\n",
    "    for key in dict_clsuters.keys():\n",
    "    \n",
    "       \n",
    "           scores[key] = evaluate_cluster_LLM(dict_clsuters[key], openaikey=openai_key)\n",
    "           \n",
    "    print(f\"mean value of the scores for each cluster is: {np.mean(scores)}\")      \n",
    "    \n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def random_search_V2(sentences, embeddings, space, num_evals):\n",
    "    \"\"\"\n",
    "    Randomly search hyperparameter space a limited number of times \n",
    "    and return a summary of the results\n",
    "    New version with the different prompt. \n",
    "    \"\"\"\n",
    "    #random.seed(101)\n",
    "    results = []\n",
    "    \n",
    "    for i in trange(num_evals):\n",
    "        n_neighbors = random.choice(space['n_neighbors'])\n",
    "        min_cluster_size = random.choice(space['min_cluster_size'])\n",
    "        min_samples = random.choice(space['min_samples'])\n",
    "        cluster_selection_epsilon = random.choice(space['cluster_selection_epsilon'])\n",
    "\n",
    "        clusters = generate_clusters(embeddings, \n",
    "                                     n_neighbors=n_neighbors, \n",
    "                                     min_cluster_size=min_cluster_size, \n",
    "                                     min_samples=min_samples,\n",
    "                                     cluster_selection_epsilon=cluster_selection_epsilon, \n",
    "                                     random_state=101)\n",
    "    \n",
    "        label_count, dbcv = dbcv_score(clusters, prob_threshold=0.05)\n",
    "        \n",
    "        \n",
    "        llm_score = evaluate_multiple_clusters_LLM(sentences, clusters)\n",
    "        #llm_score = 1\n",
    "        cost = (dbcv + llm_score) /2. \n",
    "        print(f\"Final score of the cluster {cost}\")\n",
    "        \n",
    "        results.append([i, n_neighbors, min_cluster_size, min_samples, cluster_selection_epsilon,\n",
    "                        label_count, dbcv, llm_score, cost])\n",
    "    \n",
    "    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', \n",
    "                                               'min_cluster_size', 'min_samples', 'cluster_selection_epsilon', \n",
    "                                               'label_count','dbcv', 'llm_score', 'final_cost'])\n",
    "    result_df = result_df.sort_values(by='final_cost', ascending=False)\n",
    "\n",
    "    best_params = result_df.iloc[0]\n",
    "\n",
    "    # Extract the best parameters as a dictionary\n",
    "    best_params_dict = {\n",
    "        'n_neighbors': best_params['n_neighbors'],\n",
    "        'min_cluster_size': best_params['min_cluster_size'],\n",
    "        'min_samples': best_params['min_samples'],\n",
    "        'cluster_selection_epsilon': best_params['cluster_selection_epsilon'],                                 \n",
    "        'label_count': best_params['label_count'],\n",
    "        \"dbcv\": best_params['dbcv'],\n",
    "        #\"fraction_goodcluster\": best_params['fraction_goodcluster'],\n",
    "        'final_cost': best_params['final_cost']\n",
    "    }\n",
    "\n",
    "    return result_df, best_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and paragraph formation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "full_paragraphs = []\n",
    "full_sentences = []\n",
    "\n",
    "full_paragraphs_metadata = []\n",
    "\n",
    "for key in sources_metadata.keys():\n",
    "    data = sources_metadata[key]\n",
    "    title= data['title']\n",
    "    content = data['content']\n",
    "    url = data['url']\n",
    "\n",
    "#for text in pdf_texts:\n",
    "#print(text)\n",
    "    processed_text = process_text(content)\n",
    "        \n",
    "    sentences = get_sentences(processed_text)\n",
    "    sentences = remove_bad_sentences(processed_text)\n",
    "\n",
    "    paragraphs = [\" \".join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        full_paragraphs_metadata.append({\"title\": title, \"url\": url, \"paragraph\": paragraph})\n",
    "    \n",
    "\n",
    "    # Add the sentences to the full_sentences list\n",
    "    full_sentences.extend(sentences)\n",
    "    full_paragraphs.extend(paragraphs)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_embedding = SentenceTransformer(\"nomic-ai/modernbert-embed-base\")\n",
    "\n",
    "#model_embedding = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "embedding = embed(model_embedding, full_paragraphs)\n",
    "\n",
    "continue_loop = True\n",
    "while continue_loop == True:\n",
    "    spcace_parameters = get_space_parameters(nsentences = len(full_paragraphs))\n",
    "\n",
    "    costs_rdnsearch, best_params = random_search_V2(full_paragraphs, embedding, spcace_parameters, 1)\n",
    "    print(f\"number of clusters {costs_rdnsearch.iloc[0]['label_count']}\")\n",
    "    if costs_rdnsearch.iloc[0]['label_count'] >= 6: continue_loop = False\n",
    "    \n",
    "clusters = generate_clusters(embedding, \n",
    "                                    n_neighbors = int(best_params['n_neighbors']), \n",
    "                                    min_cluster_size = int(best_params['min_cluster_size']), \n",
    "                                    min_samples = int(best_params['min_samples']),\n",
    "                                    cluster_selection_epsilon = float(best_params['cluster_selection_epsilon']),                              \n",
    "                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_rdnsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_clusters(full_paragraphs, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{folder_cluster_headline_path}/params-clusters-txt\", exist_ok=True)\n",
    "costs_rdnsearch.iloc[0].to_csv(f\"{folder_cluster_headline_path}/params-clusters-txt/{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = display_clusters(full_paragraphs, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#clusters_dict = display_clusters(full_paragraphs, clusters)\n",
    "\n",
    "\n",
    "clusters_dict = {int(k): v for k, v in clusters_dict.items()}\n",
    "\n",
    "\n",
    "#cluster_path = f\"./Results/Clusters/clusters-{week}-{sector}.json\"\n",
    "os.makedirs(f\"{folder_cluster_path}\", exist_ok=True)\n",
    "cluster_path = f\"{folder_cluster_path}/cluster-{file_name}.json\"\n",
    "\n",
    "with open(cluster_path, \"w\") as f: \n",
    "    json.dump(clusters_dict, f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import cluster json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(cluster_path, \"r\") as f:\n",
    "    clusters_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline generation for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def generate_headline(cluster, openaikey, temperature = 0.1):\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You will receive a cluster of sentences, produce me a title that best describe the cluster. \n",
    "The text are taken from huminatarian sources. \n",
    "Only return the title, no additional text. Here is the cluster: \n",
    "{cluster}\n",
    "\n",
    "\"\"\"\n",
    "    client = openai.OpenAI(api_key= openaikey)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=35,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract the model's score\n",
    "    response = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_data = display_clusters(full_paragraphs, clusters)\n",
    "output_dict = {}\n",
    "for cluster_num, sentences in clusters_data.items():\n",
    "    headline = generate_headline(sentences, openai_key)\n",
    "    print(\"Headline generated correctly\")\n",
    "    cluster_info = {\n",
    "        \"cluster_articles\": [{\"id\": f\"article_id_{cluster_num}.txt\", \"text\": sentence} for sentence in sentences],\n",
    "        \"cluster_headline\": headline\n",
    "    }\n",
    "    output_dict[int(cluster_num)] = cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline + cluster saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "#cluster_headline_path = f\"./Results/Clusters+Headline /clusters-headlines-{week}-{sector}.json\"\n",
    "cluster_headline_path = f\"{folder_cluster_headline_path}/clusters-{file_name}.json\"\n",
    "\n",
    "with open(cluster_headline_path, 'w') as handle:\n",
    "    json.dump(output_dict, handle)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(cluster_headline_path, \"r\") as f:\n",
    "        headline_data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

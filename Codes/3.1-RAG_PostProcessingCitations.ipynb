{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede57c39",
   "metadata": {},
   "source": [
    "# Post-processing of Generated Citations\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs the **post-processing** of the citations generated in the previous step, refining and structuring the outputs as needed.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **path variables** to specify the input and output directories used throughout the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "\n",
    "file_name = \"Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024-prompt-1\"\n",
    "\n",
    "answer_path_one_file = f\"./Results/Answers/Answers-subtopics/Dev set/answes-{file_name}.json\"\n",
    "\n",
    "\n",
    "\n",
    "output_path = Path(\"./Results/Answers/Answers-subtopics/Dev set/Updated_citations\")\n",
    "output_path_one_file = output_path / f\"answers_new_cit-{file_name}.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2535cf",
   "metadata": {},
   "source": [
    "# Open Answers file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(answer_path_one_file, 'r') as f:\n",
    "    answers_file = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a4d46",
   "metadata": {},
   "source": [
    "# Filter file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c3098",
   "metadata": {},
   "source": [
    "I filter the questions which have not a generated answer and in the ones in which I have one I remove the retrieved docs that are not used in the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(answers_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw loaded JSON data to filter items with citations\n",
    "    and associate citations with their corresponding contexts.\n",
    "\n",
    "    Args:\n",
    "        answers_file (list): The list of dictionaries loaded from the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary has 'citations'\n",
    "              and 'used_contexts' keys added.\n",
    "    \"\"\"\n",
    "    filtered_file = []\n",
    "    for item in answers_file:\n",
    "        answer = item.get('retrieved_answer', '')\n",
    "        # Only include items that potentially have citations\n",
    "        if \"[\" in answer or \"]\" in answer:\n",
    "            filtered_file.append(item)\n",
    "\n",
    "    preprocessed_items = []\n",
    "    for item in filtered_file:\n",
    "        citations = []\n",
    "        answer = item.get('retrieved_answer', '')\n",
    "        retrieved_context = item.get('retrieved_contexts', [])\n",
    "\n",
    "        # Find all matches like [1], [2], etc.\n",
    "        matches_str = re.findall(r'\\[\\s*(\\d+)\\s*\\]', answer)\n",
    "\n",
    "        if matches_str:\n",
    "            citations_int = list(map(int, matches_str))\n",
    "            unique_sorted_citations = sorted(set(citations_int))\n",
    "            citations = unique_sorted_citations\n",
    "\n",
    "        used_contexts = []\n",
    "        for i in citations:\n",
    "            index = i - 1  # Adjust for 0-based indexing\n",
    "            if 0 <= index < len(retrieved_context):\n",
    "                used_contexts.append(retrieved_context[index])\n",
    "            else:\n",
    "                print(f\"⚠️ Citation [{i}] is out of bounds (retrieved_context has {len(retrieved_context)} elements).\")\n",
    "                #Suppress warning for batch processing if desired, or log it.\n",
    "\n",
    "        item['citations'] = citations\n",
    "        item['used_contexts'] = used_contexts\n",
    "        preprocessed_items.append(item)\n",
    "    return preprocessed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_file = preprocess_data(answers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f23be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647432e5",
   "metadata": {},
   "source": [
    "# Update citations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    tokens1 = set(text1.lower().split())\n",
    "    tokens2 = set(text2.lower().split())\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    union = tokens1.union(tokens2)\n",
    "    return len(intersection) / len(union) if union else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def update_citations_in_clusters(\n",
    "    data_list, \n",
    "    mu=0.8, \n",
    "    threshold=0.3, \n",
    "    model_name='nomic-ai/modernbert-embed-base'):\n",
    "    \n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer model '{model_name}': {e}\")\n",
    "        # Return a copy of the original data if model loading fails\n",
    "        return [item.copy() for item in data_list], 0, 0 # Return counts as 0, 0\n",
    "\n",
    "    processed_data_list = []\n",
    "    total_original_citations = 0\n",
    "    total_changed_citations = 0\n",
    "\n",
    "    for original_cluster_data in data_list:\n",
    "        cluster = original_cluster_data.copy() # Work on a copy to avoid modifying original list elements directly\n",
    "\n",
    "        llm_response = cluster.get('retrieved_answer')\n",
    "        retrieved_docs = cluster.get('retrieved_contexts') \n",
    "        query = cluster.get('question')\n",
    "\n",
    "       \n",
    "        if not llm_response or not query:\n",
    "            cluster['updated_retrieved_answer'] = llm_response \n",
    "            cluster['new_citations'] = []\n",
    "            cluster['new_used_contexts'] = []\n",
    "            old_citations_for_cluster = cluster.pop('citations', []) \n",
    "            cluster['old_citations'] = old_citations_for_cluster\n",
    "            cluster['old_used_contexts'] = cluster.pop('used_contexts', [])\n",
    "            \n",
    "            # Count original citations for clusters that are skipped\n",
    "            total_original_citations += len(old_citations_for_cluster)\n",
    "\n",
    "            processed_data_list.append(cluster)\n",
    "            continue\n",
    "\n",
    "        # Rename original citation fields (if they exist)\n",
    "        old_citations_for_cluster = cluster.pop('citations', [])\n",
    "        cluster['old_citations'] = old_citations_for_cluster\n",
    "        cluster['old_used_contexts'] = cluster.pop('used_contexts', [])\n",
    "\n",
    "        # --- Core citation update logic ---\n",
    "        pattern = re.compile(r\"(.*?)((\\[\\d+\\])+)\", re.DOTALL)\n",
    "        llm_matches = list(pattern.finditer(llm_response))\n",
    "\n",
    "        \n",
    "        query_embedding = model.encode([query]) # Shape: (1, embedding_dim)\n",
    "        \n",
    "        # Compute cosine scores between query and all retrieved documents\n",
    "        cosine_scores = np.array([]) \n",
    "        if retrieved_docs: \n",
    "            doc_embeddings = model.encode(retrieved_docs) # Shape: (num_docs, embedding_dim)\n",
    "            # Ensure doc_embeddings is not empty before calculating cosine similarity\n",
    "            if doc_embeddings.ndim == 2 and doc_embeddings.shape[0] > 0:\n",
    "                 cosine_scores = cosine_similarity(query_embedding, doc_embeddings)[0] # Shape: (num_docs,)\n",
    "\n",
    "        updated_response_content = llm_response\n",
    "        current_offset = 0 \n",
    "        all_new_overall_citation_indices = set() \n",
    "\n",
    "        if not llm_matches: # No citation patterns found in the response\n",
    "            cluster['updated_retrieved_answer'] = llm_response\n",
    "            cluster['new_citations'] = []\n",
    "            cluster['new_used_contexts'] = []\n",
    "        else:\n",
    "            for match in llm_matches:\n",
    "                text_piece_from_match = match.group(1) \n",
    "                text_piece_stripped = text_piece_from_match.strip()\n",
    "                original_citation_markers_str = match.group(2) \n",
    "                \n",
    "                # Determine k: the number of original citation markers for this piece\n",
    "                original_indices_for_this_piece = [int(num) for num in re.findall(r'\\d+', original_citation_markers_str)]\n",
    "                k = len(original_indices_for_this_piece)\n",
    "                \n",
    "                new_citation_markers_for_this_piece_str = \"\" \n",
    "                current_piece_new_valid_indices = []\n",
    "\n",
    "                if k == 0: # No valid numeric citations found in the original marker\n",
    "                    pass # new_citation_markers_for_this_piece_str remains \"\", effectively removing non-numeric markers\n",
    "                elif not retrieved_docs or cosine_scores.size == 0: \n",
    "                    # No documents to cite against, or query-doc scores couldn't be computed (e.g., all docs empty)\n",
    "                    pass # new_citation_markers_for_this_piece_str remains \"\", removing citations\n",
    "                else:\n",
    "                    # Score this text_piece against all retrieved_docs\n",
    "                    piece_document_scores = []\n",
    "                    for j, doc_text_content in enumerate(retrieved_docs):\n",
    "                        jaccard_sim = jaccard_similarity(text_piece_stripped, doc_text_content)\n",
    "                        \n",
    "                        cosine_sim_query_doc = cosine_scores[j] \n",
    "                        combined_score = mu * jaccard_sim + (1 - mu) * cosine_sim_query_doc\n",
    "                        piece_document_scores.append((combined_score, j + 1)) \n",
    "                    \n",
    "                    \n",
    "                    piece_document_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "                    \n",
    "                    \n",
    "                    top_k_scored_docs = piece_document_scores[:k]\n",
    "                    \n",
    "                    # Filter these top_k by the threshold\n",
    "                    current_piece_new_valid_indices = [\n",
    "                        doc_idx for score, doc_idx in top_k_scored_docs if score >= threshold\n",
    "                    ]\n",
    "                    \n",
    "                    if not current_piece_new_valid_indices and top_k_scored_docs:\n",
    "                        current_piece_new_valid_indices = [top_k_scored_docs[0][1]]\n",
    "                    \n",
    "                    # Sort the final new citation indices for this piece\n",
    "                    current_piece_new_valid_indices.sort()\n",
    "                    new_citation_markers_for_this_piece_str = ''.join([f'[{idx}]' for idx in current_piece_new_valid_indices])\n",
    "\n",
    "                \n",
    "                for idx in current_piece_new_valid_indices:\n",
    "                    all_new_overall_citation_indices.add(idx)\n",
    "                \n",
    "                \n",
    "                replacement_text_segment = text_piece_stripped + new_citation_markers_for_this_piece_str\n",
    "                \n",
    "                # Apply replacement in the updated_response_content string\n",
    "                match_original_start_pos = match.start()\n",
    "                match_original_end_pos = match.end()\n",
    "                \n",
    "                updated_response_content = updated_response_content[:match_original_start_pos + current_offset] + \\\n",
    "                                           replacement_text_segment + \\\n",
    "                                           updated_response_content[match_original_end_pos + current_offset:]\n",
    "                \n",
    "               \n",
    "                current_offset += len(replacement_text_segment) - (match_original_end_pos - match_original_start_pos)\n",
    "\n",
    "            # Finalize results for the cluster\n",
    "            cluster['updated_retrieved_answer'] = updated_response_content\n",
    "            \n",
    "            final_sorted_new_citation_indices = sorted(list(all_new_overall_citation_indices))\n",
    "            cluster['new_citations'] = final_sorted_new_citation_indices\n",
    "            \n",
    "            \n",
    "            if retrieved_docs:\n",
    "                cluster['new_used_contexts'] = [\n",
    "                    retrieved_docs[doc_idx-1] for doc_idx in final_sorted_new_citation_indices \n",
    "                    if 0 < doc_idx <= len(retrieved_docs) \n",
    "                ]\n",
    "            else:\n",
    "                cluster['new_used_contexts'] = []\n",
    "\n",
    "        processed_data_list.append(cluster)\n",
    "        \n",
    "        \n",
    "        total_original_citations += len(old_citations_for_cluster)\n",
    "        if set(cluster['new_citations']) != set(old_citations_for_cluster):\n",
    "            total_changed_citations += 1\n",
    "            \n",
    "    return processed_data_list, total_original_citations, total_changed_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file, tot_citations, changed_citations = update_citations_in_clusters(filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddce64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "\n",
    "\n",
    "\n",
    "with open(output_path_one_file, 'w') as f:\n",
    "    json.dump(new_file, f)\n",
    "\n",
    "\n",
    "import json \n",
    "with open(output_path_one_file, 'r') as f:\n",
    "    new_file= json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

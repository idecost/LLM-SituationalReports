{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50229a14",
   "metadata": {},
   "source": [
    "# Final Report Postprocessing\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook updateds the citations for all the generated notebooks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path: Path, data):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e588d",
   "metadata": {},
   "source": [
    "# Change citations QA Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_file(file_path: Path, output_folder: Path):\n",
    "    try:\n",
    "        data = load_json(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {file_path} non è un JSON valido ({e})\")\n",
    "        return\n",
    "\n",
    "    if \"clusters\" not in data and \"summary_contexts\" not in data:\n",
    "        print(f\"[skip] {file_path} non ha 'clusters' né 'summary_contexts'\")\n",
    "        return\n",
    "\n",
    "    context_to_data: Dict[str, dict] = {}\n",
    "    cit_pattern = re.compile(r\"\\[(\\d+)\\]\")\n",
    "\n",
    "    # 1) Raccogli tutti i contesti (sia da clusters che da summary_contexts)\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        for qa in cluster.get(\"questions_and_answers\", []):\n",
    "            for ctx in qa.get(\"used_contexts\", {}).values():\n",
    "                norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "                if norm_text not in context_to_data:\n",
    "                    context_to_data[norm_text] = ctx\n",
    "\n",
    "    for ctx in data.get(\"summary_contexts\", {}).values():\n",
    "        norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "        if norm_text not in context_to_data:\n",
    "            context_to_data[norm_text] = ctx\n",
    "\n",
    "    # 2) Scansiona i testi (clusters + summary) per assegnare numeri in ordine di apparizione\n",
    "    context_to_newnum: Dict[str, int] = {}\n",
    "    next_num = 1\n",
    "\n",
    "    def register_from_text(text: str, uc_dict: Dict[str, dict]):\n",
    "        nonlocal next_num\n",
    "        for m in cit_pattern.finditer(text or \"\"):\n",
    "            oldnum = m.group(1)\n",
    "            if oldnum in uc_dict:\n",
    "                norm_text = normalize_text(uc_dict[oldnum][\"context\"])\n",
    "            else:\n",
    "                norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "            if norm_text not in context_to_newnum:\n",
    "                context_to_newnum[norm_text] = next_num\n",
    "                next_num += 1\n",
    "\n",
    "    # cluster answers\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        for qa in cluster.get(\"questions_and_answers\", []):\n",
    "            register_from_text(qa.get(\"updated_retrieved_answer\", \"\"), qa.get(\"used_contexts\", {}))\n",
    "\n",
    "    # summary\n",
    "    if \"summary\" in data:\n",
    "        register_from_text(data[\"summary\"], data.get(\"summary_contexts\", {}))\n",
    "\n",
    "    # 3) Aggiungi contesti non citati\n",
    "    for norm_text in context_to_data:\n",
    "        if norm_text not in context_to_newnum:\n",
    "            context_to_newnum[norm_text] = next_num\n",
    "            next_num += 1\n",
    "\n",
    "    # 4) Funzione sostituzione citazioni\n",
    "    def replace_citation_match(match, uc_dict):\n",
    "        oldnum = match.group(1)\n",
    "        if oldnum in uc_dict:\n",
    "            norm_text = normalize_text(uc_dict[oldnum][\"context\"])\n",
    "        else:\n",
    "            norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "        return f\"[{context_to_newnum[norm_text]}]\"\n",
    "\n",
    "    # Aggiorna cluster answers\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        for qa in cluster.get(\"questions_and_answers\", []):\n",
    "            if \"updated_retrieved_answer\" in qa:\n",
    "                qa[\"updated_retrieved_answer\"] = cit_pattern.sub(\n",
    "                    lambda m: replace_citation_match(m, qa.get(\"used_contexts\", {})),\n",
    "                    qa[\"updated_retrieved_answer\"] or \"\"\n",
    "                )\n",
    "            new_uc = {}\n",
    "            for old_key, ctx in qa.get(\"used_contexts\", {}).items():\n",
    "                norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "                new_key = str(context_to_newnum[norm_text])\n",
    "                new_uc[new_key] = ctx\n",
    "            qa[\"used_contexts\"] = new_uc\n",
    "\n",
    "    # Aggiorna summary\n",
    "    if \"summary\" in data:\n",
    "        data[\"summary\"] = cit_pattern.sub(\n",
    "            lambda m: replace_citation_match(m, data.get(\"summary_contexts\", {})),\n",
    "            data[\"summary\"] or \"\"\n",
    "        )\n",
    "\n",
    "    # Aggiorna summary_contexts\n",
    "    if \"summary_contexts\" in data:\n",
    "        new_sc = {}\n",
    "        for old_key, ctx in data[\"summary_contexts\"].items():\n",
    "            norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "            new_key = str(context_to_newnum[norm_text])\n",
    "            new_sc[new_key] = ctx\n",
    "        data[\"summary_contexts\"] = new_sc\n",
    "        \n",
    "    # duplicate_cit_pattern = re.compile(r\"(\\[\\d+\\])\\1+\") \n",
    "    \n",
    "    # def remove_adjacent_duplicates(text: str) -> str:\n",
    "    #     # Sostituisce la sequenza di citazioni duplicate adiacenti con la prima occorrenza\n",
    "    #     # es: \"[1][1][1]\" viene sostituito con \"[1]\"\n",
    "    #     return duplicate_cit_pattern.sub(r\"\\1\", text or \"\")\n",
    "        \n",
    "    # # Applica la rimozione su cluster answers\n",
    "    # for cluster in data.get(\"clusters\", []):\n",
    "    #     for qa in cluster.get(\"questions_and_answers\", []):\n",
    "    #         if \"updated_retrieved_answer\" in qa:\n",
    "    #             qa[\"updated_retrieved_answer\"] = remove_adjacent_duplicates(qa[\"updated_retrieved_answer\"])\n",
    "\n",
    "    # # Applica la rimozione su summary\n",
    "    # if \"summary\" in data:\n",
    "    #     data[\"summary\"] = remove_adjacent_duplicates(data[\"summary\"])\n",
    "    \n",
    "    # 5) Salva (il numero del passo è rimasto 5 per coerenza con l'originale se si considera il nuovo come un'aggiunta)\n",
    "    output_path = output_folder / file_path.name\n",
    "    save_json(output_path, data)\n",
    "    print(f\"[ok] salvato aggiornato: {output_path}\")\n",
    "\n",
    "    # 5) Salva\n",
    "    output_path = output_folder / file_path.name\n",
    "    save_json(output_path, data)\n",
    "    print(f\"[ok] salvato aggiornato: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1972e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_all(folder_path: Path):\n",
    "    output_folder = folder_path / \"updated_citations_V2\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    for file_path in folder_path.glob(\"*.json\"):\n",
    "        reindex_file(file_path, output_folder)\n",
    "\n",
    "folder_path = Path(\"./Results/Reports/JSON_Report_QA/Dev set\")\n",
    "reindex_all(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15bcdc",
   "metadata": {},
   "source": [
    "## SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29557adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Set\n",
    "\n",
    "CIT_PATTERN = re.compile(r\"\\[(\\d+)\\]\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize context text so identical contexts map to the same key.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.casefold()  # case-insensitive stable normalization\n",
    "\n",
    "def load_json(p: Path):\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(p: Path, data):\n",
    "    with p.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def reindex_file(file_path: Path, output_folder: Path):\n",
    "    try:\n",
    "        data = load_json(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {file_path} non è un JSON valido ({e})\")\n",
    "        return\n",
    "\n",
    "    # Identify top-level categories (list-of-QA) while preserving insertion order.\n",
    "    # Exclude known metadata fields.\n",
    "    metadata_keys = {\"file_name\", \"summary\", \"summary_contexts\", \"clusters\"}\n",
    "    category_keys = []\n",
    "    for k, v in data.items():\n",
    "        if k in metadata_keys:\n",
    "            continue\n",
    "        if isinstance(v, list):\n",
    "            # accept empty lists and lists whose elements look like QAs\n",
    "            if not v or (isinstance(v[0], dict) and (\"question\" in v[0] or \"retrieved_answer\" in v[0] or \"used_contexts\" in v[0])):\n",
    "                category_keys.append(k)\n",
    "\n",
    "    # Gather contexts globally (norm_text -> context dict)\n",
    "    context_to_data: Dict[str, dict] = {}\n",
    "    \n",
    "    global_old_to_norms: Dict[str, Set[str]] = {}\n",
    "\n",
    "    def register_context(old_key: str, ctx: dict):\n",
    "        norm = normalize_text(ctx.get(\"context\", \"\"))\n",
    "        # Only register non-empty contexts\n",
    "        if norm:\n",
    "            if norm not in context_to_data:\n",
    "                context_to_data[norm] = ctx\n",
    "            if old_key is not None:\n",
    "                global_old_to_norms.setdefault(str(old_key), set()).add(norm)\n",
    "\n",
    "    # From clusters\n",
    "    for cluster in data.get(\"clusters\", []) if isinstance(data.get(\"clusters\"), list) else []:\n",
    "        for qa in cluster.get(\"questions_and_answers\", []):\n",
    "            for oldk, ctx in qa.get(\"used_contexts\", {}).items():\n",
    "                register_context(oldk, ctx)\n",
    "\n",
    "    # From summary_contexts\n",
    "    for oldk, ctx in (data.get(\"summary_contexts\") or {}).items():\n",
    "        register_context(oldk, ctx)\n",
    "\n",
    "    # From category-style QAs\n",
    "    for cat in category_keys:\n",
    "        for qa in data.get(cat, []):\n",
    "            for oldk, ctx in qa.get(\"used_contexts\", {}).items():\n",
    "                register_context(oldk, ctx)\n",
    "\n",
    "    # Helper to resolve an old numeric ref to a normalized text key (best-effort)\n",
    "    def resolve_old_to_norm(oldnum: str, local_uc: Dict[str, dict]):\n",
    "        # 1) local used_contexts (most reliable)\n",
    "        if local_uc and oldnum in local_uc:\n",
    "            norm = normalize_text(local_uc[oldnum].get(\"context\", \"\"))\n",
    "            if norm:  # only return if non-empty\n",
    "                return norm\n",
    "        # 2) summary_contexts (useful when scanning summary)\n",
    "        if oldnum in (data.get(\"summary_contexts\") or {}):\n",
    "            norm = normalize_text((data[\"summary_contexts\"][oldnum] or {}).get(\"context\", \"\"))\n",
    "            if norm:\n",
    "                return norm\n",
    "        # 3) if this oldnum has been seen globally and maps to exactly one normalized context, use that\n",
    "        if oldnum in global_old_to_norms and len(global_old_to_norms[oldnum]) == 1:\n",
    "            return next(iter(global_old_to_norms[oldnum]))\n",
    "        # 4) fallback: return None for missing/empty contexts (will be filtered later)\n",
    "        return None\n",
    "\n",
    "    # Assign new numbers in order of first appearance when scanning the texts top-to-bottom (file order)\n",
    "    context_to_newnum: Dict[str, int] = {}\n",
    "    next_num = 1\n",
    "\n",
    "    def register_from_text(text: str, local_uc: Dict[str, dict]):\n",
    "        nonlocal next_num\n",
    "        if not text:\n",
    "            return\n",
    "        for m in CIT_PATTERN.finditer(text):\n",
    "            old = m.group(1)\n",
    "            norm = resolve_old_to_norm(old, local_uc or {})\n",
    "            if norm and norm not in context_to_newnum:\n",
    "                context_to_newnum[norm] = next_num\n",
    "                next_num += 1\n",
    "\n",
    "    # Iterate top-level items in insertion order so numbering follows file appearance\n",
    "    for key, val in data.items():\n",
    "        if key == \"clusters\":\n",
    "            for cluster in val:\n",
    "                for qa in cluster.get(\"questions_and_answers\", []):\n",
    "                    ans = qa.get(\"updated_retrieved_answer\") or qa.get(\"retrieved_answer\") or \"\"\n",
    "                    register_from_text(ans, qa.get(\"used_contexts\", {}))\n",
    "        elif key == \"summary\":\n",
    "            register_from_text(val or \"\", data.get(\"summary_contexts\", {}))\n",
    "        elif key in category_keys:\n",
    "            for qa in val:\n",
    "                ans = qa.get(\"updated_retrieved_answer\") or qa.get(\"retrieved_answer\") or \"\"\n",
    "                register_from_text(ans, qa.get(\"used_contexts\", {}))\n",
    "        # else metadata or other -> ignore for numbering\n",
    "\n",
    "    # Add any contexts that were never cited so they still get a unique number\n",
    "    for norm in context_to_data:\n",
    "        if norm not in context_to_newnum:\n",
    "            context_to_newnum[norm] = next_num\n",
    "            next_num += 1\n",
    "\n",
    "    # Replacement helper that uses the same resolution used while registering\n",
    "    def replace_match_with_newnum(match, local_uc: Dict[str, dict]):\n",
    "        old = match.group(1)\n",
    "        norm = resolve_old_to_norm(old, local_uc or {})\n",
    "        if norm is None:\n",
    "            # Missing/empty context -> remove citation\n",
    "            return \"\"\n",
    "        newnum = context_to_newnum.get(norm)\n",
    "        if newnum is None:\n",
    "            # shouldn't happen, but guard\n",
    "            return \"\"\n",
    "        return f\"[{newnum}]\"\n",
    "\n",
    "    # Update all QA answers and their used_contexts maps\n",
    "    # Clusters\n",
    "    for cluster in data.get(\"clusters\", []) if isinstance(data.get(\"clusters\"), list) else []:\n",
    "        for qa in cluster.get(\"questions_and_answers\", []):\n",
    "            # replace in answer text (prefer updated_retrieved_answer if present)\n",
    "            if \"updated_retrieved_answer\" in qa:\n",
    "                qa[\"updated_retrieved_answer\"] = CIT_PATTERN.sub(\n",
    "                    lambda m: replace_match_with_newnum(m, qa.get(\"used_contexts\", {})),\n",
    "                    qa.get(\"updated_retrieved_answer\") or \"\"\n",
    "                )\n",
    "            else:\n",
    "                # fallback to retrieved_answer if updated not present\n",
    "                if \"retrieved_answer\" in qa:\n",
    "                    qa[\"retrieved_answer\"] = CIT_PATTERN.sub(\n",
    "                        lambda m: replace_match_with_newnum(m, qa.get(\"used_contexts\", {})),\n",
    "                        qa.get(\"retrieved_answer\") or \"\"\n",
    "                    )\n",
    "\n",
    "            # rebuild used_contexts with new numeric keys (one entry per unique normalized context)\n",
    "            new_uc = {}\n",
    "            for oldk, ctx in qa.get(\"used_contexts\", {}).items():\n",
    "                norm = normalize_text(ctx.get(\"context\", \"\"))\n",
    "                if norm:  # only include non-empty contexts\n",
    "                    new_key = str(context_to_newnum[norm])\n",
    "                    # keep the first occurrence for that new_key\n",
    "                    if new_key not in new_uc:\n",
    "                        new_uc[new_key] = ctx\n",
    "            qa[\"used_contexts\"] = new_uc\n",
    "\n",
    "    # Category-style QAs\n",
    "    for cat in category_keys:\n",
    "        for qa in data.get(cat, []):\n",
    "            # update answer text (these files use `retrieved_answer` typically)\n",
    "            if \"updated_retrieved_answer\" in qa:\n",
    "                qa[\"updated_retrieved_answer\"] = CIT_PATTERN.sub(\n",
    "                    lambda m: replace_match_with_newnum(m, qa.get(\"used_contexts\", {})),\n",
    "                    qa.get(\"updated_retrieved_answer\") or \"\"\n",
    "                )\n",
    "            else:\n",
    "                qa[\"retrieved_answer\"] = CIT_PATTERN.sub(\n",
    "                    lambda m: replace_match_with_newnum(m, qa.get(\"used_contexts\", {})),\n",
    "                    qa.get(\"retrieved_answer\") or \"\"\n",
    "                )\n",
    "\n",
    "            new_uc = {}\n",
    "            for oldk, ctx in qa.get(\"used_contexts\", {}).items():\n",
    "                norm = normalize_text(ctx.get(\"context\", \"\"))\n",
    "                if norm:  # only include non-empty contexts\n",
    "                    new_key = str(context_to_newnum[norm])\n",
    "                    if new_key not in new_uc:\n",
    "                        new_uc[new_key] = ctx\n",
    "            qa[\"used_contexts\"] = new_uc\n",
    "\n",
    "    # Update summary text & summary_contexts if present\n",
    "    if \"summary\" in data:\n",
    "        data[\"summary\"] = CIT_PATTERN.sub(\n",
    "            lambda m: replace_match_with_newnum(m, data.get(\"summary_contexts\", {})),\n",
    "            data.get(\"summary\") or \"\"\n",
    "        )\n",
    "\n",
    "    if \"summary_contexts\" in data:\n",
    "        new_sc = {}\n",
    "        for oldk, ctx in (data.get(\"summary_contexts\") or {}).items():\n",
    "            norm = normalize_text(ctx.get(\"context\", \"\"))\n",
    "            if norm:  # only include non-empty contexts\n",
    "                new_key = str(context_to_newnum[norm])\n",
    "                if new_key not in new_sc:\n",
    "                    new_sc[new_key] = ctx\n",
    "        data[\"summary_contexts\"] = new_sc\n",
    "        \n",
    "    # Save\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_folder / file_path.name\n",
    "    save_json(output_path, data)\n",
    "    print(f\"[ok] salvato aggiornato: {output_path}\")\n",
    "\n",
    "def reindex_all(folder_path: Path, recursive: bool = False):\n",
    "    \"\"\"\n",
    "    Reindex all JSON files in folder_path, writing updated files into folder_path/updated_citations_V2.\n",
    "    Set recursive=True to traverse subfolders.\n",
    "    \"\"\"\n",
    "    output_folder = folder_path / \"updated_citations_V2\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file_path in folder_path.glob(pattern):\n",
    "        # skip files already in the output folder\n",
    "        if output_folder in file_path.parents:\n",
    "            continue\n",
    "        try:\n",
    "            reindex_file(file_path, output_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] {file_path} non è stato processato ({e})\")\n",
    "            \n",
    "            \n",
    "folder_path = Path(\"./Results/Reports/JSON_Report_QA_SDGs/Dev set\")\n",
    "reindex_all(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45f766",
   "metadata": {},
   "source": [
    "# Change citations summary clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d185378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "def remove_duplicate_citations(text: str) -> str:\n",
    "    \"\"\"Remove duplicate numeric citations like [1][2][3][2][1] → [1][2][3].\"\"\"\n",
    "    def deduplicate(match):\n",
    "        citations = match.groups()[0].split('][')\n",
    "        citations = [c.strip('[]') for c in citations]\n",
    "        seen = set()\n",
    "        unique_citations = [x for x in citations if not (x in seen or seen.add(x))]\n",
    "        return ''.join(f'[{x}]' for x in unique_citations)\n",
    "    \n",
    "    return re.sub(r'((?:\\[\\d+\\])+)', deduplicate, text or \"\")\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path: Path, data):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def reindex_file(file_path: Path, output_folder: Path):\n",
    "    try:\n",
    "        data = load_json(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {file_path} non è un JSON valido ({e})\")\n",
    "        return\n",
    "\n",
    "    if \"clusters\" not in data and \"summary_contexts\" not in data:\n",
    "        print(f\"[skip] {file_path} non ha né 'clusters' né 'summary_contexts'\")\n",
    "        return\n",
    "\n",
    "    cit_pattern = re.compile(r\"\\[(\\d+)\\]\")\n",
    "\n",
    "    # 1) Raccogli tutti i contesti normalizzati (da clusters + summary_contexts)\n",
    "    context_to_data: Dict[str, dict] = {}\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        for ctx in cluster.get(\"used_contexts\", {}).values():\n",
    "            norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "            if norm_text not in context_to_data:\n",
    "                context_to_data[norm_text] = ctx\n",
    "\n",
    "    for ctx in data.get(\"summary_contexts\", {}).values():\n",
    "        norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "        if norm_text not in context_to_data:\n",
    "            context_to_data[norm_text] = ctx\n",
    "\n",
    "    # 2) Scansiona testi (cluster_summary + summary) per assegnare numeri in ordine di apparizione\n",
    "    context_to_newnum: Dict[str, int] = {}\n",
    "    next_num = 1\n",
    "\n",
    "    def register_from_text(text: str, uc_dict: Dict[str, dict]):\n",
    "        nonlocal next_num\n",
    "        for m in cit_pattern.finditer(text or \"\"):\n",
    "            oldnum = m.group(1)\n",
    "            if oldnum in uc_dict:\n",
    "                norm_text = normalize_text(uc_dict[oldnum][\"context\"])\n",
    "            else:\n",
    "                norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "            if norm_text not in context_to_newnum:\n",
    "                context_to_newnum[norm_text] = next_num\n",
    "                next_num += 1\n",
    "\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        register_from_text(cluster.get(\"cluster_summary\", \"\"), cluster.get(\"used_contexts\", {}))\n",
    "\n",
    "    if \"summary\" in data:\n",
    "        register_from_text(data[\"summary\"], data.get(\"summary_contexts\", {}))\n",
    "\n",
    "    # 3) Aggiungi contesti non citati\n",
    "    for norm_text in context_to_data:\n",
    "        if norm_text not in context_to_newnum:\n",
    "            context_to_newnum[norm_text] = next_num\n",
    "            next_num += 1\n",
    "\n",
    "    # 4) Funzione per sostituire le citazioni\n",
    "    def replace_citation_match(match, uc_dict):\n",
    "        oldnum = match.group(1)\n",
    "        if oldnum in uc_dict:\n",
    "            norm_text = normalize_text(uc_dict[oldnum][\"context\"])\n",
    "        else:\n",
    "            norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "        return f\"[{context_to_newnum[norm_text]}]\"\n",
    "\n",
    "    # Aggiorna cluster_summary\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        if \"cluster_summary\" in cluster:\n",
    "            cluster[\"cluster_summary\"] = cit_pattern.sub(\n",
    "                lambda m: replace_citation_match(m, cluster.get(\"used_contexts\", {})),\n",
    "                cluster[\"cluster_summary\"] or \"\"\n",
    "            )\n",
    "            \n",
    "            # Remove duplicate citations\n",
    "            \n",
    "            cluster[\"cluster_summary\"] = remove_duplicate_citations(cluster[\"cluster_summary\"])\n",
    "            \n",
    "            \n",
    "        new_uc = {}\n",
    "        for old_key, ctx in cluster.get(\"used_contexts\", {}).items():\n",
    "            norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "            new_key = str(context_to_newnum[norm_text])\n",
    "            new_uc[new_key] = ctx\n",
    "        cluster[\"used_contexts\"] = new_uc\n",
    "\n",
    "    # Aggiorna summary\n",
    "    if \"summary\" in data:\n",
    "        data[\"summary\"] = cit_pattern.sub(\n",
    "            lambda m: replace_citation_match(m, data.get(\"summary_contexts\", {})),\n",
    "            data[\"summary\"] or \"\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        data[\"summary\"] = remove_duplicate_citations(data[\"summary\"])\n",
    "\n",
    "    # Aggiorna summary_contexts\n",
    "    if \"summary_contexts\" in data:\n",
    "        new_sc = {}\n",
    "        for old_key, ctx in data[\"summary_contexts\"].items():\n",
    "            norm_text = normalize_text(ctx.get(\"context\", \"\"))\n",
    "            new_key = str(context_to_newnum[norm_text])\n",
    "            new_sc[new_key] = ctx\n",
    "        data[\"summary_contexts\"] = new_sc\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    # 5) Salva\n",
    "    output_path = output_folder / file_path.name\n",
    "    save_json(output_path, data)\n",
    "    print(f\"[ok] salvato aggiornato: {output_path}\")\n",
    "\n",
    "\n",
    "def reindex_all(folder_path: Path):\n",
    "    output_folder = folder_path / \"updated_citations_V2\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    for file_path in folder_path.glob(\"*.json\"):\n",
    "        reindex_file(file_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cluster_path = Path(\"./Results/Reports/JSON_Report_Summaries/Dev set\")\n",
    "reindex_all(summary_cluster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88d58d",
   "metadata": {},
   "source": [
    "## SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2023e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalizes whitespace in a string.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def load_json(path: Path) -> Any:\n",
    "    \"\"\"Loads JSON data from a file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path: Path, data: Any):\n",
    "    \"\"\"Saves data to a JSON file, creating parent directories if needed.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def get_context_text(ctx: Any) -> str:\n",
    "    \"\"\"Safely extracts context text, handling both dict and string formats.\"\"\"\n",
    "    if isinstance(ctx, dict):\n",
    "        return ctx.get(\"context\", \"\")\n",
    "    elif isinstance(ctx, str):\n",
    "        return ctx\n",
    "    return \"\"\n",
    "\n",
    "def reindex_file(file_path: Path, output_folder: Path):\n",
    "    print(file_path)\n",
    "    try:\n",
    "        data = load_json(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {file_path} non è un JSON valido ({e})\")\n",
    "        return\n",
    "\n",
    "    if \"clusters\" not in data and \"summary_contexts\" not in data:\n",
    "        print(f\"[skip] {file_path} non ha né 'clusters' né 'summary_contexts'\")\n",
    "        return\n",
    "\n",
    "    cit_pattern = re.compile(r\"\\[(\\d+)\\]\")\n",
    "\n",
    "    # 1) Raccogli tutti i contesti normalizzati (da clusters + summary_contexts)\n",
    "    context_to_data: Dict[str, dict] = {}\n",
    "    \n",
    "    # Process contexts from clusters\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        for ctx in cluster.get(\"used_contexts\", {}).values():\n",
    "            raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "            norm_text = normalize_text(raw_text)\n",
    "            if norm_text and norm_text not in context_to_data:\n",
    "                context_to_data[norm_text] = ctx\n",
    "\n",
    "    # Process contexts from summary_contexts\n",
    "    for ctx in data.get(\"summary_contexts\", {}).values():\n",
    "        raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "        norm_text = normalize_text(raw_text)\n",
    "        if norm_text and norm_text not in context_to_data:\n",
    "            context_to_data[norm_text] = ctx\n",
    "\n",
    "    # 2) Scansiona testi (cluster_summary + summary) per assegnare numeri in ordine di apparizione\n",
    "    context_to_newnum: Dict[str, int] = {}\n",
    "    next_num = 1\n",
    "\n",
    "    def register_from_text(text: str, uc_dict: Dict[str, Any]):\n",
    "        nonlocal next_num\n",
    "        for m in cit_pattern.finditer(text or \"\"):\n",
    "            oldnum = m.group(1)\n",
    "            if oldnum in uc_dict:\n",
    "                ctx = uc_dict[oldnum]\n",
    "                raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "                norm_text = normalize_text(raw_text)\n",
    "            else:\n",
    "                norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "            \n",
    "            if norm_text not in context_to_newnum:\n",
    "                context_to_newnum[norm_text] = next_num\n",
    "                next_num += 1\n",
    "\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        register_from_text(cluster.get(\"cluster_summary\", \"\"), cluster.get(\"used_contexts\", {}))\n",
    "\n",
    "    if \"summary\" in data:\n",
    "        register_from_text(data[\"summary\"], data.get(\"summary_contexts\", {}))\n",
    "\n",
    "    # 3) Aggiungi contesti non citati\n",
    "    for norm_text in context_to_data:\n",
    "        if norm_text not in context_to_newnum:\n",
    "            context_to_newnum[norm_text] = next_num\n",
    "            next_num += 1\n",
    "\n",
    "    # 4) Funzione per sostituire le citazioni\n",
    "    def replace_citation_match(match, uc_dict):\n",
    "        oldnum = match.group(1)\n",
    "        if oldnum in uc_dict:\n",
    "            ctx = uc_dict[oldnum]\n",
    "            raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "            norm_text = normalize_text(raw_text)\n",
    "        else:\n",
    "            norm_text = f\"__MISSING_CONTEXT__{oldnum}\"\n",
    "        \n",
    "        # Guard against unhandled missing contexts, though all contexts should be registered by step 3\n",
    "        if norm_text not in context_to_newnum:\n",
    "            return f\"[MISSING_NEW_INDEX:{norm_text}]\"\n",
    "            \n",
    "        return f\"[{context_to_newnum[norm_text]}]\"\n",
    "\n",
    "    # Aggiorna cluster_summary e used_contexts\n",
    "    for cluster in data.get(\"clusters\", []):\n",
    "        if \"cluster_summary\" in cluster:\n",
    "            cluster[\"cluster_summary\"] = cit_pattern.sub(\n",
    "                lambda m: replace_citation_match(m, cluster.get(\"used_contexts\", {})),\n",
    "                cluster[\"cluster_summary\"] or \"\"\n",
    "            )\n",
    "            \n",
    "            cluster['cluster_summary'] = remove_duplicate_citations(cluster['cluster_summary'])\n",
    "        new_uc = {}\n",
    "        for old_key, ctx in cluster.get(\"used_contexts\", {}).items():\n",
    "            raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "            norm_text = normalize_text(raw_text)\n",
    "            \n",
    "            if norm_text in context_to_newnum:\n",
    "                new_key = str(context_to_newnum[norm_text])\n",
    "                new_uc[new_key] = ctx\n",
    "        cluster[\"used_contexts\"] = new_uc\n",
    "\n",
    "    # Aggiorna summary\n",
    "    if \"summary\" in data:\n",
    "        data[\"summary\"] = cit_pattern.sub(\n",
    "            lambda m: replace_citation_match(m, data.get(\"summary_contexts\", {})),\n",
    "            data[\"summary\"] or \"\"\n",
    "        )\n",
    "        \n",
    "        data['summary'] = remove_duplicate_citations(data['summary'])\n",
    "\n",
    "    # Aggiorna summary_contexts\n",
    "    if \"summary_contexts\" in data:\n",
    "        new_sc = {}\n",
    "        for old_key, ctx in data[\"summary_contexts\"].items():\n",
    "            raw_text = get_context_text(ctx) # <-- FIX: Use safe getter\n",
    "            norm_text = normalize_text(raw_text)\n",
    "            \n",
    "            if norm_text in context_to_newnum:\n",
    "                new_key = str(context_to_newnum[norm_text])\n",
    "                new_sc[new_key] = ctx\n",
    "        data[\"summary_contexts\"] = new_sc\n",
    "        \n",
    "    # Commented out block for duplicate citation removal\n",
    "    # 5) Salva\n",
    "    output_path = output_folder / file_path.name\n",
    "    save_json(output_path, data)\n",
    "    print(f\"[ok] salvato aggiornato: {output_path}\")\n",
    "\n",
    "\n",
    "def reindex_all(folder_path: Path):\n",
    "    output_folder = folder_path / \"updated_citations_V3\"\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    for file_path in folder_path.glob(\"*.json\"):\n",
    "        reindex_file(file_path, output_folder)\n",
    " \n",
    " \n",
    "summary_cluster_path = Path(\"./Results/Reports/JSON_Report_Summaries_SDGs/Dev set\")\n",
    "reindex_all(summary_cluster_path) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa521d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"./Results/Reports/JSON_Report_Summaries_SDGs/Dev set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a75b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70808288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

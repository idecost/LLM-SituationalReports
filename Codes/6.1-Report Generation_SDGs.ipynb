{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7f65f5",
   "metadata": {},
   "source": [
    "# Final Report Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook compiles the **final report** by combining all the results produced across the various folders.  \n",
    "In addition, **metadata** is added to each retrieved paragraph to ensure completeness and traceability.\n",
    "\n",
    "Two report formats are generated:\n",
    "1. A **Question–Answer (QA)** version.  \n",
    "2. A **summary-based** version organized by **Sustainable Development Goals (SDGs)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23d44ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_answers_data(answers_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw loaded JSON data to filter items that:\n",
    "    - Contain brackets (indicating citations)\n",
    "    - Do NOT contain 'no clear answer'\n",
    "\n",
    "    Args:\n",
    "        answers_file (list): The list of dictionaries loaded from the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries that meet the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_file = []\n",
    "    for item in answers_file:\n",
    "        answer = item.get('updated_answer_pt2', '')\n",
    "        \n",
    "        has_brackets = \"[\" in answer or \"]\" in answer\n",
    "        has_no_clear = re.search(r\"no clear answer\", answer, re.IGNORECASE)\n",
    "\n",
    "        if has_brackets and not has_no_clear:\n",
    "            filtered_file.append(item)\n",
    "\n",
    "    return filtered_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24898fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_answers_data(answers_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw loaded JSON data to filter items that:\n",
    "    - Contain brackets (indicating citations)\n",
    "    - Do NOT contain 'no clear answer'\n",
    "\n",
    "    Args:\n",
    "        answers_file (dict): Dict of {sdg: list of QA dicts}.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict with the same keys, but filtered lists of QA dicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_file = {}\n",
    "    for sdg_key, qa_list in answers_file.items():\n",
    "        filtered_list = []\n",
    "        for item in qa_list:\n",
    "            # Try both updated_answer_pt2 (if present) and retrieved_answer (fallback)\n",
    "            answer = item.get('updated_answer_pt2', item.get('retrieved_answer', ''))\n",
    "\n",
    "            has_brackets = \"[\" in answer or \"]\" in answer\n",
    "            has_no_clear = re.search(r\"no clear answer\", answer, re.IGNORECASE)\n",
    "\n",
    "            if has_brackets and not has_no_clear:\n",
    "                filtered_list.append(item)\n",
    "\n",
    "        if filtered_list:  # only keep non-empty SDGs\n",
    "            filtered_file[sdg_key] = filtered_list\n",
    "\n",
    "    return filtered_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1aff3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary_data(summary_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw loaded JSON data to filter items with citations\n",
    "    and associate citations with their corresponding contexts.\n",
    "\n",
    "    Args:\n",
    "        answers_file (list): The list of dictionaries loaded from the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary has 'citations'\n",
    "              and 'used_contexts' keys added.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_file = {}\n",
    "    for key, answer in summary_file.items():\n",
    "        # Only include items that potentially have citations\n",
    "        if \"[\" in answer or \"]\" in answer or re.search(r\"no clear answer[^\\w]*\", answer, re.IGNORECASE):\n",
    "            filtered_file[key] = answer\n",
    "    return filtered_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e2a8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_name(s):\n",
    "    \"\"\"\n",
    "    Parses a string in the format 'Country_event-period'\n",
    "    and returns a dictionary with keys: country, event, period.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        country_event, period = s.split(\"-\", 1)  # split only on first '-'\n",
    "        country, event = country_event.split(\"_\", 1)  # split only on first '_'\n",
    "        \n",
    "        return country,event,period\n",
    "    except ValueError:\n",
    "        raise ValueError(\"String does not match expected format: 'Country_event-period'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "268789e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enrich_contexts_with_citation_numbers(context_list, metadata_data, answer_text):\n",
    "#     \"\"\"\n",
    "#     context_list: list of paragraph texts\n",
    "#     metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "#     answer_text: the answer text containing citation numbers like [2][4]\n",
    "#     Returns a dict keyed by citation numbers.\n",
    "#     \"\"\"\n",
    "#     citation_numbers = np.unique(re.findall(r'\\[(\\d+)\\]', answer_text))\n",
    "#     enriched_contexts = {}\n",
    "\n",
    "#     for idx, citation in enumerate(citation_numbers):\n",
    "#         if idx < len(context_list):\n",
    "#             context_text = context_list[idx]\n",
    "#             matched_metadata = next(\n",
    "#                 (item for item in metadata_data if item.get('paragraph', '') == context_text),\n",
    "#                 None\n",
    "#             )\n",
    "#             enriched_contexts[citation] = {\n",
    "#                 \"context\": context_text,\n",
    "#                 \"title\": matched_metadata.get(\"title\", \"\") if matched_metadata else \"\",\n",
    "#                 \"url\": matched_metadata.get(\"url\", \"\") if matched_metadata else \"\"\n",
    "#             }\n",
    "#         else:\n",
    "#             enriched_contexts[citation] = {\"context\": \"\", \"title\": \"\", \"url\": \"\"}\n",
    "\n",
    "#     return enriched_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7533e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# def enrich_contexts_with_citation_numbers(context_list, metadata_data, answer_text, similarity_threshold=0.7):\n",
    "#     \"\"\"\n",
    "#     context_list: list of paragraph texts\n",
    "#     metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "#     answer_text: the answer text containing citation numbers like [2][4]\n",
    "#     similarity_threshold: minimum ratio (0–1) to accept fuzzy match\n",
    "#     Returns a dict keyed by citation numbers.\n",
    "#     \"\"\"\n",
    "#     # Extract citation numbers from the answer text\n",
    "#     citation_numbers = np.unique(re.findall(r'\\[(\\d+)\\]', answer_text))\n",
    "#     enriched_contexts = {}\n",
    "\n",
    "#     # Helper: get best fuzzy match\n",
    "#     def find_best_match(context_text):\n",
    "#         best_match = None\n",
    "#         best_ratio = 0.0\n",
    "#         for item in metadata_data:\n",
    "#             candidate_text = item.get('paragraph', '')\n",
    "#             ratio = SequenceMatcher(None, context_text, candidate_text).ratio()\n",
    "#             if ratio > best_ratio:\n",
    "#                 best_ratio = ratio\n",
    "#                 best_match = item\n",
    "#         return best_match if best_ratio >= similarity_threshold else None\n",
    "\n",
    "#     # Step 1: Assign contexts to citations\n",
    "#     for idx, citation in enumerate(citation_numbers):\n",
    "#         if idx < len(context_list):\n",
    "#             context_text = context_list[idx]\n",
    "\n",
    "#             # Try exact match first\n",
    "#             matched_metadata = next(\n",
    "#                 (item for item in metadata_data if item.get('paragraph', '') == context_text),\n",
    "#                 None\n",
    "#             )\n",
    "\n",
    "#             # If no exact match, try fuzzy\n",
    "#             if not matched_metadata:\n",
    "#                 matched_metadata = find_best_match(context_text)\n",
    "\n",
    "#             enriched_contexts[citation] = {\n",
    "#                 \"context\": context_text,\n",
    "#                 \"title\": matched_metadata.get(\"title\", \"\") if matched_metadata else \"\",\n",
    "#                 \"url\": matched_metadata.get(\"url\", \"\") if matched_metadata else \"\"\n",
    "#             }\n",
    "#         else:\n",
    "#             # Step 2: No context available → still keep citation with placeholders\n",
    "#             enriched_contexts[citation] = {\n",
    "#                 \"context\": \"\",\n",
    "#                 \"title\": \"\",\n",
    "#                 \"url\": \"\"\n",
    "#             }\n",
    "\n",
    "#     return enriched_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2922c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import random\n",
    "\n",
    "def enrich_contexts_with_citation_numbers(context_list, metadata_data, answer_text, similarity_threshold=0.7, min_substring_length=50, use_fuzzy=True):\n",
    "    \"\"\"\n",
    "    context_list: list of paragraph texts\n",
    "    metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "    answer_text: the answer text containing citation numbers like [2][4]\n",
    "    similarity_threshold: minimum ratio (0–1) to accept fuzzy match\n",
    "    min_substring_length: minimum length for substring matching (avoids false positives)\n",
    "    use_fuzzy: whether to use fuzzy matching (can be slow on large datasets)\n",
    "    Returns a dict keyed by citation numbers.\n",
    "    \"\"\"\n",
    "    # Extract citation numbers from the answer text\n",
    "    citation_numbers = np.unique(re.findall(r'\\[(\\d+)\\]', answer_text))\n",
    "    enriched_contexts = {}\n",
    "\n",
    "    # Pre-process metadata for faster lookups\n",
    "    # Create exact match lookup\n",
    "    exact_lookup = {item.get('paragraph', ''): item for item in metadata_data}\n",
    "    \n",
    "    # Create normalized substring lookup (only for substring matching)\n",
    "    substring_lookup = []\n",
    "    for item in metadata_data:\n",
    "        para = item.get('paragraph', '')\n",
    "        if para:\n",
    "            substring_lookup.append({\n",
    "                'normalized': para.lower().strip(),\n",
    "                'original': item\n",
    "            })\n",
    "\n",
    "    # Helper: get best fuzzy match (only called when needed)\n",
    "    def find_best_match(context_text):\n",
    "        best_match = None\n",
    "        best_ratio = 0.0\n",
    "        for item in metadata_data:\n",
    "            candidate_text = item.get('paragraph', '')\n",
    "            ratio = SequenceMatcher(None, context_text, candidate_text).ratio()\n",
    "            if ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = item\n",
    "        return best_match if best_ratio >= similarity_threshold else None\n",
    "\n",
    "    # Helper: find substring matches (optimized)\n",
    "    def find_substring_match(context_text):\n",
    "        \"\"\"\n",
    "        Check if context_text is contained within any metadata paragraph.\n",
    "        Returns first match found (or random if multiple).\n",
    "        \"\"\"\n",
    "        if len(context_text) < min_substring_length:\n",
    "            return None\n",
    "            \n",
    "        context_normalized = context_text.lower().strip()\n",
    "        matches = []\n",
    "        \n",
    "        # Simple substring search - much faster than fuzzy matching\n",
    "        for item in substring_lookup:\n",
    "            if context_normalized in item['normalized']:\n",
    "                matches.append(item['original'])\n",
    "                # Optionally break early if you only need one match\n",
    "                # break\n",
    "        \n",
    "        return random.choice(matches) if matches else None\n",
    "\n",
    "    # Step 1: Assign contexts to citations\n",
    "    for idx, citation in enumerate(citation_numbers):\n",
    "        if idx < len(context_list):\n",
    "            context_text = context_list[idx]\n",
    "            matched_metadata = None\n",
    "\n",
    "            # Try exact match first (O(1) lookup)\n",
    "            matched_metadata = exact_lookup.get(context_text)\n",
    "\n",
    "            # If no exact match, try substring matching (faster than fuzzy)\n",
    "            if not matched_metadata:\n",
    "                matched_metadata = find_substring_match(context_text)\n",
    "\n",
    "            # If still no match and fuzzy is enabled, try fuzzy whole-document match (slowest)\n",
    "            if not matched_metadata and use_fuzzy:\n",
    "                matched_metadata = find_best_match(context_text)\n",
    "\n",
    "            enriched_contexts[citation] = {\n",
    "                \"context\": context_text,\n",
    "                \"title\": matched_metadata.get(\"title\", \"\") if matched_metadata else \"\",\n",
    "                \"url\": matched_metadata.get(\"url\", \"\") if matched_metadata else \"\"\n",
    "            }\n",
    "        else:\n",
    "            # Step 2: No context available → still keep citation with placeholders\n",
    "            enriched_contexts[citation] = {\n",
    "                \"context\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"url\": \"\"\n",
    "            }\n",
    "\n",
    "    return enriched_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08e58d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# def enrich_summary_with_citation_numbers(summary_text, cited_paragraphs, metadata_data, similarity_threshold=0.7):\n",
    "#     \"\"\"\n",
    "#     summary_text: str, the summary string containing citations like [1][2]\n",
    "#     cited_paragraphs: list of paragraph texts\n",
    "#     metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "#     similarity_threshold: minimum similarity ratio (0–1) to accept fuzzy match\n",
    "#     Returns a dict keyed by citation numbers.\n",
    "#     \"\"\"\n",
    "#     # Extract all citation numbers from the summary text\n",
    "#     citation_numbers = np.unique(re.findall(r'\\[(\\d+)\\]', summary_text))\n",
    "#     print(\"Summary citations found:\", citation_numbers)\n",
    "\n",
    "#     enriched_contexts = {}\n",
    "\n",
    "#     # Helper: find best fuzzy match\n",
    "#     def find_best_match(context_text):\n",
    "#         best_match = None\n",
    "#         best_ratio = 0.0\n",
    "#         for item in metadata_data:\n",
    "#             candidate_text = item.get(\"paragraph\", \"\")\n",
    "#             ratio = SequenceMatcher(None, context_text, candidate_text).ratio()\n",
    "#             if ratio > best_ratio:\n",
    "#                 best_ratio = ratio\n",
    "#                 best_match = item\n",
    "#         return best_match if best_ratio >= similarity_threshold else None\n",
    "\n",
    "#     # Step 1: Assign contexts to citations\n",
    "#     for idx, citation in enumerate(citation_numbers):\n",
    "#         if idx < len(cited_paragraphs):\n",
    "#             context_text = cited_paragraphs[idx]\n",
    "\n",
    "#             # Try exact match first\n",
    "#             matched_metadata = next(\n",
    "#                 (item for item in metadata_data if item.get(\"paragraph\", \"\") == context_text),\n",
    "#                 None\n",
    "#             )\n",
    "\n",
    "#             # If no exact match, try fuzzy matching\n",
    "#             if not matched_metadata:\n",
    "#                 matched_metadata = find_best_match(context_text)\n",
    "\n",
    "#             enriched_contexts[citation] = {\n",
    "#                 \"context\": context_text,\n",
    "#                 \"title\": matched_metadata.get(\"title\", \"\") if matched_metadata else \"\",\n",
    "#                 \"url\": matched_metadata.get(\"url\", \"\") if matched_metadata else \"\"\n",
    "#             }\n",
    "#         else:\n",
    "#             # No paragraph for this citation\n",
    "#             enriched_contexts[citation] = {\n",
    "#                 \"context\": \"\",\n",
    "#                 \"title\": \"\",\n",
    "#                 \"url\": \"\"\n",
    "#             }\n",
    "\n",
    "#     return enriched_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "225f9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_names = {\n",
    "    \"SDG-1\": \"No Poverty\",\n",
    "    \"SDG-2\": \"Zero Hunger\",\n",
    "    \"SDG-3\": \"Good Health and Well-being\",\n",
    "    \"SDG-4\": \"Quality Education\",\n",
    "    \"SDG-5\": \"Gender Equality\",\n",
    "    \"SDG-6\": \"Clean Water and Sanitation\",\n",
    "    \"SDG-7\": \"Affordable and Clean Energy\",\n",
    "    \"SDG-8\": \"Decent Work and Economic Growth\",\n",
    "    \"SDG-9\": \"Industry, Innovation and Infrastructure\",\n",
    "    \"SDG-10\": \"Reduced Inequalities\",\n",
    "    \"SDG-11\": \"Sustainable Cities and Communities\",\n",
    "    \"SDG-12\": \"Responsible Consumption and Production\",\n",
    "    \"SDG-13\": \"Climate Action\",\n",
    "    \"SDG-14\": \"Life Below Water\",\n",
    "    \"SDG-15\": \"Life on Land\",\n",
    "    \"SDG-16\": \"Peace, Justice and Strong Institutions\",\n",
    "    \"SDG-17\": \"Partnerships for the Goals\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "893c2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sdg_sort_key(key: str):\n",
    "    \"\"\"Extracts the number from keys like 'SDG-1: No Poverty' for correct sorting.\"\"\"\n",
    "    match = re.search(r\"SDG-(\\d+)\", key)\n",
    "    return int(match.group(1)) if match else 999  # push unknowns to the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a923655",
   "metadata": {},
   "source": [
    "# Create reports QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad2dba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Names: ['Afghanistan_Afghanistan Floods-Week 21 2024', 'Bangladesh_Cyclone Remal-Week 21 2024', 'Haiti_Gang violence and humanitarian crisis in Haiti-Week 40 2024', 'India_LandslideFloods-Week 31 2024', 'Indonesia_Floods and volcanic activity in Indonesia-Week 20 2024', 'Israel_Israel-Hamas war-Week 19 2024', 'Israel_Israel_Palestine_confilct-Week 40 2024', 'Jamaica_Hurricane Beryl-Week 28 2024', 'Nigeria_Flooding in Nigeria-Week 37 2024', 'Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024', 'Sudan_Sudan conflict-Week 34 2024', 'Sudan_Sudan conflict-Week 39 2024', 'Ukraine_Ukraine-Week 23 2024', 'United Kingdom_UK riots-Week 32 2024']\n",
      "\n",
      "answers_new_cit-answes-Afghanistan_Afghanistan Floods-Week 21 2024-prompt-1.json -> original: 14, filtered: 14, removed: 0\n",
      "Summary citations found: ['17' '32' '39' '46' '64' '7']\n",
      "answers_new_cit-answes-Bangladesh_Cyclone Remal-Week 21 2024-prompt-1.json -> original: 13, filtered: 12, removed: 1\n",
      "Summary citations found: ['11' '34' '48' '63' '67' '70' '72' '73' '74' '77']\n",
      "answers_new_cit-answes-Haiti_Gang violence and humanitarian crisis in Haiti-Week 40 2024-prompt-1.json -> original: 14, filtered: 13, removed: 1\n",
      "Summary citations found: ['124' '129' '135' '136' '138' '139' '142' '3' '41' '63' '75']\n",
      "answers_new_cit-answes-India_LandslideFloods-Week 31 2024-prompt-1.json -> original: 13, filtered: 11, removed: 2\n",
      "Summary citations found: ['1' '34' '42' '49' '52' '57' '69' '92']\n",
      "answers_new_cit-answes-Indonesia_Floods and volcanic activity in Indonesia-Week 20 2024-prompt-1.json -> original: 10, filtered: 9, removed: 1\n",
      "Summary citations found: ['11' '12' '20' '21' '22' '23' '24' '7']\n",
      "answers_new_cit-answes-Israel_Israel-Hamas war-Week 19 2024-prompt-1.json -> original: 11, filtered: 11, removed: 0\n",
      "Summary citations found: ['12' '17' '19' '27' '30' '7']\n",
      "answers_new_cit-answes-Israel_Israel_Palestine_confilct-Week 40 2024-prompt-1.json -> original: 12, filtered: 12, removed: 0\n",
      "Summary citations found: ['50' '55' '6' '73' '81' '98']\n",
      "answers_new_cit-answes-Jamaica_Hurricane Beryl-Week 28 2024-prompt-1.json -> original: 14, filtered: 14, removed: 0\n",
      "Summary citations found: ['20' '25' '32' '36' '43' '54' '62']\n",
      "answers_new_cit-answes-Nigeria_Flooding in Nigeria-Week 37 2024-prompt-1.json -> original: 12, filtered: 11, removed: 1\n",
      "Summary citations found: ['1' '18' '46' '52' '72' '75' '9']\n",
      "answers_new_cit-answes-Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024-prompt-1.json -> original: 11, filtered: 9, removed: 2\n",
      "Summary citations found: ['10' '13' '14' '15' '17' '18' '25']\n",
      "answers_new_cit-answes-Sudan_Sudan conflict-Week 34 2024-prompt-1.json -> original: 11, filtered: 11, removed: 0\n",
      "Summary citations found: ['1' '19' '30' '31' '39' '40' '43' '45' '7' '8']\n",
      "QA file not found: answers_new_cit-answes-Sudan_Sudan conflict-Week 39 2024-prompt-1.json\n",
      "Metadata file not found for Sudan_Sudan conflict-Week 39 2024: metadata-sources-metadata-Sudan_Sudan conflict-Week 39 2024.json\n",
      "Summary file not found: summary-Sudan_Sudan conflict-Week 39 2024.json\n",
      "answers_new_cit-answes-Ukraine_Ukraine-Week 23 2024-prompt-1.json -> original: 15, filtered: 15, removed: 0\n",
      "Summary citations found: ['28' '46' '49' '5' '79' '83' '87' '89' '90' '92']\n",
      "answers_new_cit-answes-United Kingdom_UK riots-Week 32 2024-prompt-1.json -> original: 5, filtered: 5, removed: 0\n",
      "Summary citations found: ['26' '37' '59' '68' '75' '80' '88']\n",
      "\n",
      "--- Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- SDG Names Dictionary ---\n",
    "sdg_names = {\n",
    "    \"SDG-1\": \"No Poverty\",\n",
    "    \"SDG-2\": \"Zero Hunger\",\n",
    "    \"SDG-3\": \"Good Health and Well-being\",\n",
    "    \"SDG-4\": \"Quality Education\",\n",
    "    \"SDG-5\": \"Gender Equality\",\n",
    "    \"SDG-6\": \"Clean Water and Sanitation\",\n",
    "    \"SDG-7\": \"Affordable and Clean Energy\",\n",
    "    \"SDG-8\": \"Decent Work and Economic Growth\",\n",
    "    \"SDG-9\": \"Industry, Innovation and Infrastructure\",\n",
    "    \"SDG-10\": \"Reduced Inequalities\",\n",
    "    \"SDG-11\": \"Sustainable Cities and Communities\",\n",
    "    \"SDG-12\": \"Responsible Consumption and Production\",\n",
    "    \"SDG-13\": \"Climate Action\",\n",
    "    \"SDG-14\": \"Life Below Water\",\n",
    "    \"SDG-15\": \"Life on Land\",\n",
    "    \"SDG-16\": \"Peace, Justice and Strong Institutions\",\n",
    "    \"SDG-17\": \"Partnerships for the Goals\"\n",
    "}\n",
    "# --- End SDG Names Dictionary ---\n",
    "\n",
    "\n",
    "devOrTest = \"Dev\"\n",
    "\n",
    "# Define your source and cluster folders using f-strings\n",
    "sources_folder = f\"./Results/Sources/SourcesCountryEvent/{devOrTest} set/\"\n",
    "qa_folder = f\"./Results/Answers/Answers-SDGs\"\n",
    "summary_folder = f\"./Results/Executive Summaries/Dev set/Updated_citations\"\n",
    "metadata_folder = \"./Results/paragraphs_metadata\"\n",
    "\n",
    "output_folder = f\"./Results/Reports/JSON_Report_QA_SDGs/{devOrTest} set\"\n",
    "markdown_output_folder = f\"./Results/Reports/Markdown_Report_QA_SDGs/{devOrTest} set\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(markdown_output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get all JSON file names from the sources folder, sorted\n",
    "json_files = np.sort([f for f in os.listdir(sources_folder) if f.endswith('.json')])\n",
    "\n",
    "# Create an array of base names (without the .json extension)\n",
    "base_names = [os.path.splitext(f)[0] for f in json_files]\n",
    "print(f\"Base Names: {base_names}\\n\")\n",
    "\n",
    "for json_file in json_files:\n",
    "    base_name = os.path.splitext(json_file)[0]\n",
    "\n",
    "    # Load QA data\n",
    "    qa_name = f\"answers_new_cit-answes-{base_name}-prompt-1.json\"\n",
    "    data_qa = {}\n",
    "    try:\n",
    "        with open(os.path.join(qa_folder, qa_name), 'r') as file:\n",
    "            data_qa = json.load(file)\n",
    "            before = len(data_qa)\n",
    "\n",
    "            data_qa = preprocess_answers_data(data_qa)\n",
    "            after = len(data_qa)\n",
    "\n",
    "            print(f\"{qa_name} -> original: {before}, filtered: {after}, removed: {before - after}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"QA file not found: {qa_name}\")\n",
    "        data_qa = {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading QA {qa_name}: {e}\")\n",
    "        data_qa = {}\n",
    "\n",
    "    # Load metadata\n",
    "    metadata_file_name = f\"metadata-sources-metadata-{base_name}.json\"\n",
    "    metadata_data = []\n",
    "    try:\n",
    "        with open(os.path.join(metadata_folder, metadata_file_name), 'r', encoding='utf-8') as file:\n",
    "            metadata_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Metadata file not found for {base_name}: {metadata_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Problems reading metadata file {metadata_file_name}: {e}\")\n",
    "\n",
    "    # Load summary\n",
    "    summary_file_name = f\"summary-{base_name}.json\"\n",
    "    summary_text = \"\"\n",
    "    summary_contexts = {}\n",
    "    try:\n",
    "        with open(os.path.join(summary_folder, summary_file_name), 'r') as file:\n",
    "            data_summary = json.load(file)\n",
    "            summary_text = data_summary.get('new_summary', data_summary.get('summary', ''))\n",
    "            cited_paragraphs = data_summary.get('new_cited_paragraphs', [])\n",
    "            summary_contexts = enrich_summary_with_citation_numbers(\n",
    "                summary_text, cited_paragraphs, metadata_data\n",
    "            )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Summary file not found: {summary_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading summary {summary_file_name}: {e}\")\n",
    "\n",
    "    # Group QA by formatted SDG name and enrich contexts\n",
    "    grouped_qa = {}\n",
    "    for qa_key, qa_list in data_qa.items():\n",
    "        # qa_key is expected to be like 'sdg_1', 'SDG-2', etc.\n",
    "        formatted_key_base = qa_key.upper().replace(\"_\", \"-\")  # e.g., SDG-1\n",
    "        sdg_name_long = sdg_names.get(formatted_key_base, None)\n",
    "        \n",
    "        if sdg_name_long:\n",
    "            # Create the desired key format: \"SDG 1 - No Poverty\"\n",
    "            # Extract the number part from 'SDG-X'\n",
    "            sdg_number = formatted_key_base.split('-')[-1]\n",
    "            sdg_name_key = f\"SDG {sdg_number} - {sdg_name_long}\"\n",
    "        else:\n",
    "            # Fallback if the key isn't in sdg_names (though it should be for SDGs)\n",
    "            sdg_name_key = formatted_key_base\n",
    "\n",
    "        if qa_list:\n",
    "            enriched_list = []\n",
    "            for item in qa_list:\n",
    "                question = item.get('question', '')\n",
    "                answer_text = item.get('retrieved_answer', '')\n",
    "                original_contexts = item.get('new_used_contexts', [])\n",
    "                enriched_contexts = enrich_contexts_with_citation_numbers(\n",
    "                    original_contexts, metadata_data, answer_text\n",
    "                )\n",
    "                enriched_list.append({\n",
    "                    'question': question,\n",
    "                    'retrieved_answer': answer_text,\n",
    "                    'used_contexts': enriched_contexts\n",
    "                })\n",
    "            grouped_qa[sdg_name_key] = enriched_list\n",
    "\n",
    "    # --- Sorting Logic ---\n",
    "    # Sort SDG names numerically by extracting the number part from the key\n",
    "    def sort_key_func(sdg_key):\n",
    "        match = re.search(r'SDG (\\d+)', sdg_key)\n",
    "        return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "    sorted_sdg_names = sorted(grouped_qa.keys(), key=sort_key_func)\n",
    "    # --- End Sorting Logic ---\n",
    "\n",
    "    # Build output JSON\n",
    "    output_data = {\n",
    "        'file_name': base_name,\n",
    "        'summary': summary_text,\n",
    "        'summary_contexts': summary_contexts\n",
    "    }\n",
    "    # Add SDGs in sorted order\n",
    "    for sdg_name in sorted_sdg_names:\n",
    "        output_data[sdg_name] = grouped_qa[sdg_name]\n",
    "\n",
    "    # Save JSON\n",
    "    output_file_path = os.path.join(output_folder, f\"{base_name}_combined_data.json\")\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        # Sort keys to ensure 'file_name', 'summary', 'summary_contexts' come first,\n",
    "        # followed by the numerically sorted SDG keys.\n",
    "        def json_sort_key(item):\n",
    "            key = item[0]\n",
    "            if key == 'file_name': return 0\n",
    "            if key == 'summary': return 1\n",
    "            if key == 'summary_contexts': return 2\n",
    "            match = re.search(r'SDG (\\d+)', key)\n",
    "            return int(match.group(1)) + 2 if match else float('inf')\n",
    "\n",
    "        # Use OrderedDict or a list of tuples to maintain order, but since\n",
    "        # Python 3.7+ dicts maintain insertion order, we can rely on that.\n",
    "        # We manually construct the dict in the desired order above.\n",
    "        json.dump(output_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    # --- Markdown generation ---\n",
    "    markdown_content = f\"# {base_name}\\n\\n\"\n",
    "    markdown_content += \"## Summary\\n\"\n",
    "    markdown_content += f\"{summary_text}\\n\\n\"\n",
    "\n",
    "    # Use the same sorted list for Markdown\n",
    "    for sdg_name in sorted_sdg_names:\n",
    "        markdown_content += f\"## {sdg_name}\\n\\n\"\n",
    "        for qa in grouped_qa[sdg_name]:\n",
    "            markdown_content += f\"**Question:** {qa['question']}\\n\"\n",
    "            markdown_content += f\"**Answer:** {qa['retrieved_answer']}\\n\\n\"\n",
    "\n",
    "    markdown_file_path = os.path.join(markdown_output_folder, f\"{base_name}.md\")\n",
    "    with open(markdown_file_path, 'w', encoding='utf-8') as md_file:\n",
    "        md_file.write(markdown_content)\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfd9b7",
   "metadata": {},
   "source": [
    "# Create report just with the summary in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e3346f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enrich_contexts(used_contexts, metadata_data):\n",
    "#     \"\"\"\n",
    "#     used_contexts: dict {id: paragraph_text}\n",
    "#     metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "#     \"\"\"\n",
    "#     enriched = {}\n",
    "#     for cid, context_text in used_contexts.items():\n",
    "#         matched_metadata = next(\n",
    "#             (item for item in metadata_data if item.get('paragraph', '') == context_text),\n",
    "#             None\n",
    "#         )\n",
    "#         if matched_metadata:\n",
    "#             enriched[cid] = {\n",
    "#                 \"context\": context_text,\n",
    "#                 \"title\": matched_metadata.get(\"title\", \"\"),\n",
    "#                 \"url\": matched_metadata.get(\"url\", \"\")\n",
    "#             }\n",
    "#         else:\n",
    "#             enriched[cid] = {\n",
    "#                 \"context\": context_text,\n",
    "#                 \"title\": \"\",\n",
    "#                 \"url\": \"\"\n",
    "#             }\n",
    "#     return enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "673ebdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def enrich_contexts(used_contexts, metadata_data, min_substring_length=50):\n",
    "    \"\"\"\n",
    "    used_contexts: dict {id: paragraph_text}\n",
    "    metadata_data: list of dicts [{paragraph, title, url}, ...]\n",
    "    min_substring_length: minimum length for substring matching\n",
    "    \"\"\"\n",
    "    # Pre-process metadata for faster lookups\n",
    "    # Create exact match lookup (O(1))\n",
    "    exact_lookup = {item.get('paragraph', ''): item for item in metadata_data}\n",
    "    \n",
    "    # Create normalized substring lookup\n",
    "    substring_lookup = []\n",
    "    for item in metadata_data:\n",
    "        para = item.get('paragraph', '')\n",
    "        if para:\n",
    "            substring_lookup.append({\n",
    "                'normalized': para.lower().strip(),\n",
    "                'original': item\n",
    "            })\n",
    "    \n",
    "    # Helper: find substring match\n",
    "    def find_substring_match(context_text):\n",
    "        if len(context_text) < min_substring_length:\n",
    "            return None\n",
    "            \n",
    "        context_normalized = context_text.lower().strip()\n",
    "        matches = []\n",
    "        \n",
    "        for item in substring_lookup:\n",
    "            if context_normalized in item['normalized']:\n",
    "                matches.append(item['original'])\n",
    "        \n",
    "        return random.choice(matches) if matches else None\n",
    "    \n",
    "    enriched = {}\n",
    "    for cid, context_text in used_contexts.items():\n",
    "        matched_metadata = None\n",
    "        \n",
    "        # Try exact match first (O(1))\n",
    "        matched_metadata = exact_lookup.get(context_text)\n",
    "        \n",
    "        # If no exact match, try substring matching\n",
    "        if not matched_metadata:\n",
    "            matched_metadata = find_substring_match(context_text)\n",
    "        \n",
    "        enriched[cid] = {\n",
    "            \"context\": context_text,\n",
    "            \"title\": matched_metadata.get(\"title\", \"\") if matched_metadata else \"\",\n",
    "            \"url\": matched_metadata.get(\"url\", \"\") if matched_metadata else \"\"\n",
    "        }\n",
    "    \n",
    "    return enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9cbb8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Define your source and cluster folders\n",
    "devOrTest = 'Dev'\n",
    "sources_folder = f\"./Results/Sources/SourcesCountryEvent/{devOrTest} set/\"\n",
    "clusters_folder = \"./Results/Clusters+Headline \"\n",
    "qa_folder = f\"./Results/Answers/Answers-SDG\"\n",
    "summary_folder = f\"./Results/Executive Summaries/Dev set/Updated_citations\"\n",
    "cluster_summary_folder = f\"./Results/Summaries/UniqueSummary-EachSDG/\"\n",
    "combined_output_folder = f\"./Results/Reports/JSON_Report_Summaries_SDGs/{devOrTest} set\" # Renamed output folder\n",
    "markdown_output_folder = f\"./Results/Reports/Markdown_Report_Summaries_Sdgs/{devOrTest} set\" # Renamed Markdown output folder\n",
    "\n",
    "metadata_folder = \"./Results/paragraphs_metadata\"\n",
    "\n",
    "# Ensure the output folders exist\n",
    "os.makedirs(combined_output_folder, exist_ok=True)\n",
    "os.makedirs(markdown_output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get all JSON file names from the sources folder, sorted\n",
    "# json_files = np.sort([f for f in os.listdir(sources_folder) if f.endswith('.json')])\n",
    "\n",
    "# # Create an array of base names (without the .json extension)\n",
    "# base_names = [os.path.splitext(f)[0] for f in json_files]\n",
    "# print(f\"Base Names: {base_names}\\n\")\n",
    "\n",
    "# # for json_file in json_files:\n",
    "# #     base_name = os.path.splitext(json_file)[0]\n",
    "\n",
    "# #     # --- Load overall summary ---\n",
    "# #     summary_file_name = f\"summary-{base_name}.json\"\n",
    "# #     try:\n",
    "# #         with open(os.path.join(summary_folder, summary_file_name), 'r') as file:\n",
    "# #             data_summary = json.load(file)\n",
    "# #             overall_summary = data_summary.get('summary', '')\n",
    "# #     except:\n",
    "# #         overall_summary = \"\"\n",
    "\n",
    "# #     # --- Load SDG cluster summaries ---\n",
    "# #     cluster_summary_file_name = f\"summary_{base_name}.json\"\n",
    "# #     try:\n",
    "# #         with open(os.path.join(cluster_summary_folder, cluster_summary_file_name), 'r') as file:\n",
    "# #             data_cluster_summaries = json.load(file)  # expected dict { \"sdg-1\": \"...\", \"sdg-2\": \"...\" }\n",
    "# #     except:\n",
    "# #         data_cluster_summaries = {}\n",
    "# #     # --- Load metadata -- \n",
    "# #     metadata_file_name = f\"metadata-sources-metadata-{base_name}.json\"\n",
    "# #     metadata_data = []\n",
    "# #     try:\n",
    "# #         with open(os.path.join(metadata_folder, metadata_file_name), 'r') as file:\n",
    "# #             metadata_data = json.load(file)\n",
    "# #     except FileNotFoundError:\n",
    "# #         print(f\"Metadata file not found for {base_name}: {os.path.join(metadata_folder, metadata_file_name)}\")\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Problems reading metadata file {os.path.join(metadata_folder, metadata_file_name)}: {e}\")\n",
    "        \n",
    "# #     # --- Build JSON structure grouped by SDG ---\n",
    "# #     output_data = {'file_name': base_name, 'summary': overall_summary}\n",
    "\n",
    "# #     grouped_summaries = {}\n",
    "# #     for sdg_key, text in data_cluster_summaries.items():\n",
    "# #         formatted_key = sdg_key.upper().replace(\"_\", \"-\")  # e.g., SDG-1\n",
    "# #         sdg_name = sdg_names.get(formatted_key, formatted_key)\n",
    "# #         full_key = f\"{formatted_key}: {sdg_name}\"  # e.g., \"SDG-1: No Poverty\"\n",
    "# #         if text.strip():\n",
    "# #             grouped_summaries[full_key] = text\n",
    "\n",
    "# #     # Sort keys alphabetically\n",
    "# #     sorted_keys = sorted(grouped_summaries.keys(), key=sdg_sort_key)\n",
    "# #     for key in sorted_keys:\n",
    "# #         output_data[key] = grouped_summaries[key]\n",
    "\n",
    "# #     # --- Save JSON ---\n",
    "# #     output_file_path = os.path.join(combined_output_folder, f\"{base_name}_combined_data.json\")\n",
    "# #     with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "# #         json.dump(output_data, outfile, indent=4, ensure_ascii=False)\n",
    "# #     print(f\"Created combined JSON file: {output_file_path}\")\n",
    "# for json_file in json_files:\n",
    "#     base_name = os.path.splitext(json_file)[0]\n",
    "\n",
    "#     # --- Load overall summary ---\n",
    "#     summary_file_name = f\"summary-{base_name}.json\"\n",
    "#     try:\n",
    "#         with open(os.path.join(summary_folder, summary_file_name), 'r') as file:\n",
    "#             data_summary = json.load(file)\n",
    "\n",
    "#             # NEW: use enriched version\n",
    "#             summary_text = data_summary.get('new_summary', data_summary.get('summary', ''))\n",
    "#             cited_paragraphs = data_summary.get('new_cited_paragraphs', [])\n",
    "\n",
    "#             output_data = {\n",
    "#                 'file_name': base_name,\n",
    "#                 'summary': summary_text,\n",
    "#                 'summary_contexts': enrich_summary_with_citation_numbers(\n",
    "#                     summary_text, cited_paragraphs, metadata_data\n",
    "#                 )\n",
    "#             }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Problem loading overall summary for {base_name}: {e}\")\n",
    "#         output_data = {'file_name': base_name, 'summary': \"\", 'summary_contexts': []}\n",
    "\n",
    "#     # --- Load SDG cluster summaries ---\n",
    "#     cluster_summary_file_name = f\"summary_{base_name}.json\"\n",
    "#     try:\n",
    "#         with open(os.path.join(cluster_summary_folder, cluster_summary_file_name), 'r') as file:\n",
    "#             data_cluster_summaries = json.load(file)  \n",
    "#             # expected dict { \"sdg-1\": {...}, \"sdg-2\": {...} }\n",
    "#     except:\n",
    "#         data_cluster_summaries = {}\n",
    "\n",
    "#     # --- Load metadata -- \n",
    "#     metadata_file_name = f\"metadata-sources-metadata-{base_name}.json\"\n",
    "#     metadata_data = []\n",
    "#     try:\n",
    "#         with open(os.path.join(metadata_folder, metadata_file_name), 'r') as file:\n",
    "#             metadata_data = json.load(file)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Metadata file not found for {base_name}: {os.path.join(metadata_folder, metadata_file_name)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Problems reading metadata file {os.path.join(metadata_folder, metadata_file_name)}: {e}\")\n",
    "        \n",
    "#     # --- Build enriched clusters ---\n",
    "#     output_data['clusters'] = []\n",
    "#     if isinstance(data_cluster_summaries, dict):\n",
    "#         for cluster_id, cluster_summary_entry in data_cluster_summaries.items():\n",
    "#             cluster_summary_text = \"\"\n",
    "#             used_contexts = {}\n",
    "\n",
    "#             if isinstance(cluster_summary_entry, dict):\n",
    "#                 cluster_summary_text = cluster_summary_entry.get(\"summary\", \"\")\n",
    "#                 raw_used_contexts = cluster_summary_entry.get(\"used_contexts\", {})\n",
    "#                 used_contexts = enrich_contexts(raw_used_contexts, metadata_data) if raw_used_contexts else {}\n",
    "\n",
    "#             elif isinstance(cluster_summary_entry, str):\n",
    "#                 # old format\n",
    "#                 cluster_summary_text = cluster_summary_entry\n",
    "\n",
    "#             if cluster_summary_text.strip():\n",
    "#                 cluster_entry = {\n",
    "#                     \"cluster_id\": cluster_id,\n",
    "#                     \"cluster_summary\": cluster_summary_text,\n",
    "#                     \"used_contexts\": used_contexts\n",
    "#                 }\n",
    "#                 output_data['clusters'].append(cluster_entry) \n",
    "\n",
    "#     # --- Save JSON ---\n",
    "#     output_file_path = os.path.join(combined_output_folder, f\"{base_name}_combined_data.json\")\n",
    "#     with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "#         json.dump(output_data, outfile, indent=4, ensure_ascii=False)\n",
    "#     print(f\"Created combined JSON file: {output_file_path}\")\n",
    "\n",
    "\n",
    "#     # --- Markdown ---\n",
    "#     markdown_content = f\"# {base_name}\\n\\n\"\n",
    "#     markdown_content += \"## Summary\\n\"\n",
    "#     markdown_content += f\"{overall_summary}\\n\\n\"\n",
    "\n",
    "#     markdown_content += \"## SDG Cluster Summaries\\n\\n\"\n",
    "#     for key in sorted_keys:\n",
    "#         markdown_content += f\"### {key}\\n\\n\"\n",
    "#         markdown_content += f\"{grouped_summaries[key]}\\n\\n\"\n",
    "\n",
    "#     markdown_file_path = os.path.join(markdown_output_folder, f\"{base_name}.md\")\n",
    "#     with open(markdown_file_path, 'w', encoding='utf-8') as md_file:\n",
    "#         md_file.write(markdown_content)\n",
    "#     print(f\"Created Markdown file: {markdown_file_path}\")\n",
    "\n",
    "# print(\"\\n--- Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e7c084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary citations found: ['17' '32' '39' '46' '64' '7']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Afghanistan_Afghanistan Floods-Week 21 2024_combined_data.json\n",
      "Summary citations found: ['11' '34' '48' '63' '67' '70' '72' '73' '74' '77']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Bangladesh_Cyclone Remal-Week 21 2024_combined_data.json\n",
      "Summary citations found: ['124' '129' '135' '136' '138' '139' '142' '3' '41' '63' '75']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Haiti_Gang violence and humanitarian crisis in Haiti-Week 40 2024_combined_data.json\n",
      "Summary citations found: ['1' '34' '42' '49' '52' '57' '69' '92']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/India_LandslideFloods-Week 31 2024_combined_data.json\n",
      "Summary citations found: ['11' '12' '20' '21' '22' '23' '24' '7']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Indonesia_Floods and volcanic activity in Indonesia-Week 20 2024_combined_data.json\n",
      "Summary citations found: ['12' '17' '19' '27' '30' '7']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Israel_Israel-Hamas war-Week 19 2024_combined_data.json\n",
      "Summary citations found: ['50' '55' '6' '73' '81' '98']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Israel_Israel_Palestine_confilct-Week 40 2024_combined_data.json\n",
      "Summary citations found: ['20' '25' '32' '36' '43' '54' '62']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Jamaica_Hurricane Beryl-Week 28 2024_combined_data.json\n",
      "Summary citations found: ['1' '18' '46' '52' '72' '75' '9']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Nigeria_Flooding in Nigeria-Week 37 2024_combined_data.json\n",
      "Summary citations found: ['10' '13' '14' '15' '17' '18' '25']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024_combined_data.json\n",
      "Summary citations found: ['1' '19' '30' '31' '39' '40' '43' '45' '7' '8']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Sudan_Sudan conflict-Week 34 2024_combined_data.json\n",
      "Metadata file not found for Sudan_Sudan conflict-Week 39 2024: ./Results/paragraphs_metadata/metadata-sources-metadata-Sudan_Sudan conflict-Week 39 2024.json\n",
      "Problem loading overall summary for Sudan_Sudan conflict-Week 39 2024: [Errno 2] No such file or directory: './Results/Executive Summaries/Dev set/Updated_citations/summary-Sudan_Sudan conflict-Week 39 2024.json'\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Sudan_Sudan conflict-Week 39 2024_combined_data.json\n",
      "Summary citations found: ['28' '46' '49' '5' '79' '83' '87' '89' '90' '92']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/Ukraine_Ukraine-Week 23 2024_combined_data.json\n",
      "Summary citations found: ['26' '37' '59' '68' '75' '80' '88']\n",
      "Created combined JSON file: ./Results/Reports/JSON_Report_Summaries_SDGs/Dev set/United Kingdom_UK riots-Week 32 2024_combined_data.json\n",
      "\n",
      "--- Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "for json_file in json_files:\n",
    "    base_name = os.path.splitext(json_file)[0]\n",
    "\n",
    "    # --- Load metadata --- \n",
    "    metadata_file_name = f\"metadata-sources-metadata-{base_name}.json\"\n",
    "    metadata_data = []\n",
    "    try:\n",
    "        with open(os.path.join(metadata_folder, metadata_file_name), 'r') as file:\n",
    "            metadata_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Metadata file not found for {base_name}: {os.path.join(metadata_folder, metadata_file_name)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Problems reading metadata file {os.path.join(metadata_folder, metadata_file_name)}: {e}\")\n",
    "\n",
    "    # --- Load overall summary ---\n",
    "    summary_file_name = f\"summary-{base_name}.json\"\n",
    "    try:\n",
    "        with open(os.path.join(summary_folder, summary_file_name), 'r') as file:\n",
    "            data_summary = json.load(file)\n",
    "\n",
    "            # Prefer enriched fields if available\n",
    "            summary_text = data_summary.get('new_summary', data_summary.get('summary', ''))\n",
    "            cited_paragraphs = data_summary.get('new_cited_paragraphs', [])\n",
    "\n",
    "            output_data = {\n",
    "                'file_name': base_name,\n",
    "                'summary': summary_text,\n",
    "                'summary_contexts': enrich_summary_with_citation_numbers(\n",
    "                    summary_text, cited_paragraphs, metadata_data\n",
    "                )\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Problem loading overall summary for {base_name}: {e}\")\n",
    "        output_data = {'file_name': base_name, 'summary': \"\", 'summary_contexts': []}\n",
    "\n",
    "    # --- Load SDG cluster summaries ---\n",
    "    cluster_summary_file_name = f\"summary_{base_name}.json\"\n",
    "    try:\n",
    "        with open(os.path.join(cluster_summary_folder, cluster_summary_file_name), 'r') as file:\n",
    "            data_cluster_summaries = json.load(file) \n",
    "            # expected dict { \"sdg-1\": {...}, \"sdg-2\": {...} }\n",
    "    except:\n",
    "        data_cluster_summaries = {}\n",
    "\n",
    "    # --- Build enriched clusters ---\n",
    "    temp_clusters = []\n",
    "    if isinstance(data_cluster_summaries, dict):\n",
    "        for cluster_id, cluster_summary_entry in data_cluster_summaries.items():\n",
    "            cluster_summary_text = \"\"\n",
    "            used_contexts = {}\n",
    "\n",
    "            # Determine the display name for the SDG\n",
    "            # Normalize key: 'sdg-1' -> 'SDG-1'\n",
    "            normalized_key = cluster_id.upper().replace(\"_\", \"-\")\n",
    "            sdg_name_long = sdg_names.get(normalized_key)\n",
    "            \n",
    "            if sdg_name_long:\n",
    "                # Extract the number part (e.g., '1' from 'SDG-1')\n",
    "                sdg_number_match = re.search(r'\\d+', normalized_key)\n",
    "                sdg_number = sdg_number_match.group(0) if sdg_number_match else \"Unknown\"\n",
    "                # Create the desired key format: \"SDG 1 - No Poverty\"\n",
    "                display_cluster_id = f\"SDG {sdg_number} - {sdg_name_long}\"\n",
    "            else:\n",
    "                # Fallback to the original ID if not found in the dictionary\n",
    "                display_cluster_id = cluster_id\n",
    "            \n",
    "            # Extract summary text and contexts\n",
    "            if isinstance(cluster_summary_entry, dict):\n",
    "                cluster_summary_text = cluster_summary_entry.get(\"summary\", \"\")\n",
    "                raw_used_contexts = cluster_summary_entry.get(\"used_contexts\", {})\n",
    "                used_contexts = enrich_contexts(raw_used_contexts, metadata_data) if raw_used_contexts else {}\n",
    "\n",
    "            elif isinstance(cluster_summary_entry, str):\n",
    "                # old format\n",
    "                cluster_summary_text = cluster_summary_entry\n",
    "\n",
    "            if cluster_summary_text.strip():\n",
    "                cluster_entry = {\n",
    "                    # Use the new formatted name\n",
    "                    \"cluster_id\": display_cluster_id, \n",
    "                    \"cluster_summary\": cluster_summary_text,\n",
    "                    \"used_contexts\": used_contexts\n",
    "                }\n",
    "                temp_clusters.append(cluster_entry)\n",
    "\n",
    "    # --- Sort the clusters by SDG number ---\n",
    "    def sort_cluster_key(cluster_entry):\n",
    "        # Extracts the number from the 'cluster_id' (e.g., 'SDG 1 - ...' -> 1)\n",
    "        match = re.search(r'SDG (\\d+)', cluster_entry['cluster_id'])\n",
    "        return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "    # Sort the list of cluster dictionaries\n",
    "    output_data['clusters'] = sorted(temp_clusters, key=sort_cluster_key)\n",
    "\n",
    "\n",
    "    # --- Save JSON ---\n",
    "    output_file_path = os.path.join(combined_output_folder, f\"{base_name}_combined_data.json\")\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(output_data, outfile, indent=4, ensure_ascii=False)\n",
    "    print(f\"Created combined JSON file: {output_file_path}\")\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f01e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

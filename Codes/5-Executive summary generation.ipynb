{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedf49ac",
   "metadata": {},
   "source": [
    "# Executive Summary Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook generates **executive summaries** for all files within a specified folder.  \n",
    "Starting from the **context used in the answers**, the summaries are produced using a **Retrieval-Augmented Generation (RAG)** pipeline.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **path variables** to define the input data, model configuration, and output directories used throughout the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ea547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings \n",
    "\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "import langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re \n",
    "os.environ['OPENAI_API_KEY'] = \"\" \n",
    "os.environ['GOOGLE_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "input_path = f\"./Results/Answers/Answers-subtopics/Dev set/Updated_citations\"\n",
    "output_path = f\"./Results/Executive Summary/Dev set\"\n",
    "base_index_dir = \".ragatouille/colbert/indexes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cff04",
   "metadata": {},
   "source": [
    "# Create summary for all the files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b909af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Get all JSON files in the input directory\n",
    "json_files = np.sort([f for f in os.listdir(input_path) if f.endswith('.json')])\n",
    "\n",
    "# Define the RAG Fusion components outside the loop if they don't change per file\n",
    "prompt_dq = ChatPromptTemplate(input_variables=['original_query'],\n",
    "                               messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[],template='You are a helpful assistant that generates multiple search queries based on a single input query.')),\n",
    "                                         HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (4 queries):'))])\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_dq | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "template = template = '''\n",
    "**You are an AI Assistant specialized in summarizing information based *strictly* on provided source documents.**\n",
    "\n",
    "**Your Task:**\n",
    "Carefully analyze the source documents provided in the context below. Then, generate a concise, one-paragraph summary that provides an overview of the situation described in the documents, focusing on the most important information happening in the country or situation at the given moment.\n",
    "\n",
    "**Instructions & Constraints:**\n",
    "\n",
    "1.  **Source Adherence:** Your summary MUST be based *solely* on the information explicitly stated or clearly inferable from the provided numbered sources. Do not include any information, assumptions, or interpretations not explicitly stated in the context.\n",
    "2.  **Conciseness & Overview:** Keep your summary concise, aiming for approximately one paragraph, and focused on providing a high-level understanding of the situation and its most important aspects. Aim for a narrative flow that integrates information from different sources where appropriate.\n",
    "3.  **Citation Requirement:** Every piece of information or statement in your summary should be attributed to the source(s) it came from.\n",
    "4.  **Citation Format:**\n",
    "    * Use bracketed numbers corresponding to the source number(s), like `[1]`, `[2]`, etc.\n",
    "    * Place citations *immediately* after the information they support.\n",
    "    * **If multiple sources support the *same* specific piece of information or statement, list all relevant source numbers together without spaces, like `[1][2]` or `[3][5][7]`.**\n",
    "    * Aim to associate citations with the specific sentence or clause they verify.\n",
    "    * Every summary **must** contain at least one citation.\n",
    "5.  **Handling Lack of Information:**\n",
    "    * If the provided sources contain relevant information but do not offer enough detail to form a comprehensive overview, state: `The provided sources offer limited information for a comprehensive overview.`\n",
    "    * If *none* of the sources contain information relevant to describing a situation, state: `The provided sources do not contain information relevant to describing a situation.`\n",
    "6.  Your citation numbers should correspond **accurately** to the sources provided in the `{context}` block. Do not put numbers to citations that are greater than the number of paragraphs you receive.\n",
    "\n",
    "---\n",
    "\n",
    "**Now, fulfill the request based on the context below:**\n",
    "\n",
    "\n",
    "{context}\n",
    "------\n",
    "\n",
    "**Summary:**\n",
    "'''\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "query = \"Generate a summary of the current situation. \"\n",
    "\n",
    "# for file in json_files:\n",
    "file = json_files[0]\n",
    "file_name = file.replace('answers_new_cit-answes-', '').replace('-prompt-1.json', '')\n",
    "print(f\"Processing file: {file_name}\")\n",
    "\n",
    "full_used_paragraphs = set()\n",
    "\n",
    "# Load JSON data\n",
    "with open(os.path.join(input_path, file), 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "for item in json_data:\n",
    "    full_used_paragraphs.update(item['new_used_contexts'])\n",
    "\n",
    "full_used_paragraphs_list = sorted(list(full_used_paragraphs))\n",
    "\n",
    "# Initialize RAGatouille model and index\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "index_name = f\"{file_name}-summary\"\n",
    "index_path = os.path.join(base_index_dir, index_name)\n",
    "\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    colbert = RAGPretrainedModel.from_index(index_path)\n",
    "else:    \n",
    "    index_path = colbert.index(index_name=index_name, collection=full_used_paragraphs_list)\n",
    "    print(f\"Index '{index_name}' created/updated successfully at '{index_path}'.\")\n",
    "\n",
    "retriever = colbert.as_langchain_retriever(k=20)\n",
    "\n",
    "\n",
    "ragfusion_chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# Create RAG chain using the retriever and prompt\n",
    "full_rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": ragfusion_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the RAG chain to get the answer\n",
    "answer = full_rag_fusion_chain.invoke(query)\n",
    "\n",
    "# Extract cited numbers\n",
    "matches = re.findall(r'\\[(\\d+)\\]', answer)\n",
    "extracted_numbers = set(int(num) for num in matches)\n",
    "\n",
    "# Get the actual cited paragraphs\n",
    "cited_paragraphs = []\n",
    "for number in extracted_numbers:\n",
    "    idx = number - 1 # Adjust for 0-based indexing\n",
    "    if 0 <= idx < len(full_used_paragraphs_list):\n",
    "        cited_paragraphs.append(full_used_paragraphs_list[idx])\n",
    "    else:\n",
    "        print(f\"Warning: Citation number {number} out of bounds for {file_name}. Max index is {len(full_used_paragraphs_list)}.\")\n",
    "\n",
    "\n",
    "# Prepare data for saving\n",
    "results_to_save = {\n",
    "    \"summary\": answer,\n",
    "    \"cited_paragraphs\": cited_paragraphs, \n",
    "    \"full_paragraphs\": full_used_paragraphs_list\n",
    "}\n",
    "print(f\"Number of docs {len(full_used_paragraphs_list)} for {file_name}\")\n",
    "# Define output file path\n",
    "output_file_name = f\"summary-{file_name}.json\"\n",
    "output_file_path = os.path.join(output_path, output_file_name)\n",
    "\n",
    "# Save the results\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    json.dump(results_to_save, outfile, indent=4)\n",
    "print(f\"Summary and cited paragraphs saved to {output_file_path}\\n\")\n",
    "\n",
    "print(\"All files processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with RAG Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds upon the **filtered questions** obtained from the previous stage and generates corresponding **answers** using a **Retrieval-Augmented Generation (RAG)** pipeline.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "At the beginning of the notebook, update the **variables** and **path definitions** to specify the input data, model configuration, and output directories used throughout the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "\n",
    "\n",
    "file_name = \"Pakistan_Monsoon floods and rains in Pakistan-Week 31 2024-prompt-1\"\n",
    "cluster_path_one_file = f\"./Results/Questions/Test questions different prompts/1-Questions generated/Dev set/questions-{file_name}.json\"\n",
    "questions_path_one_file = f\"./Results/Questions/Test questions different prompts/3-Filtered questions/Dev set/final_questions-{file_name}.json\"\n",
    "\n",
    "\n",
    "\n",
    "answers_path_one_file = f\"./Results/Answers/Answers-Subtopics/Dev set/answes-{file_name}.json\"\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\" \n",
    "os.environ['GOOGLE_API_KEY'] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(cluster_path_one_file, 'r') as file:\n",
    "    qc_data = json.load(file)\n",
    "    \n",
    "\n",
    "with open(questions_path_one_file, 'r') as file:\n",
    "    questions_data = json.load(file)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions taken from the github: https://github.com/canghongjian/beam_retriever/blob/main/test_model_tmp.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings \n",
    "\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "import langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with colbert from ragtatuille + EM and F1 for retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "\n",
    "#model = ChatOpenAI(model = 'gpt-4o', temperature=0)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with RAG Fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add improvment citations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from typing import List\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    formatted_str = \"\"\n",
    "    # Assign source IDs from 1 to k (your k=10 in retriever)\n",
    "    for i, doc in enumerate(docs):\n",
    "        source_id = i + 1 # Assigns 1, 2, 3...\n",
    "        formatted_str += f\"Source {source_id}:\\n\"\n",
    "        formatted_str += f\"{doc.page_content}\\n\\n\" # Only content, no source URL\n",
    "    return formatted_str.strip() # Remove any trailing newlines or spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def extract_documents_from_scored_tuples(scored_docs: List[Tuple[Document, float]]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extracts only the Document objects from a list of (Document, score) tuples.\n",
    "    This is specifically for the output format of your reciprocal_rank_fusion function.\n",
    "    \"\"\"\n",
    "    return [doc for doc, score in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "data = {'cluster_idx': [],'question': [], 'ground_truth_answer': [], 'retrieved_answer': [], 'retrieved_contexts': []} \n",
    "\n",
    "for idx, cluster in qc_data.items():\n",
    "    #questions = cluster.get('picked_questions', []) \n",
    "    if idx in(questions_data.keys()):\n",
    "        questions = questions_data[idx]\n",
    "        trueanswer = \"there is no true answer\"\n",
    "        print(questions)\n",
    "        for question in questions: \n",
    "\n",
    "            \n",
    "            documents = []\n",
    "            base_index_dir = \".ragatouille/colbert/indexes/\"\n",
    "            index_name = f\"{file_name}-{idx}\"\n",
    "            \n",
    "            index_path = os.path.join(base_index_dir, index_name)\n",
    "\n",
    "            #Check if the index already exists in the specified directory\n",
    "            \n",
    "            if os.path.exists(index_path):\n",
    "                #Load the RAG model from the existing index\n",
    "                colbert = RAGPretrainedModel.from_index(index_path)\n",
    "                # print(f\"Index '{index_name}' loaded successfully from '{index_path}'.\")\n",
    "                \n",
    "            else:   \n",
    "            #else:\n",
    "                cluster_articles = cluster.get('article_titles', [])\n",
    "                print(f\"cluster_articles: {cluster_articles}\")\n",
    "                \n",
    "                documents.extend(cluster_articles)\n",
    "\n",
    "                    \n",
    "                index_path = colbert.index(index_name=index_name, collection=documents)\n",
    "            \n",
    "\n",
    "            retriever = colbert.as_langchain_retriever(k=10)\n",
    "\n",
    "\n",
    "            prompt_dq = ChatPromptTemplate(input_variables=['original_query'],\n",
    "                                        messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[],template='You are a helpful assistant that generates multiple search queries based on a single input query.')),\n",
    "                                        HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (4 queries):'))])\n",
    "            generate_queries = (\n",
    "                prompt_dq | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "            )\n",
    "\n",
    "            #question = \" How old are you? \"\n",
    "\n",
    "\n",
    "            ragfusion_chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "    #         template =     '''\n",
    "            \n",
    "    #                 Using the provided context extracted by the retriever, write a high-quality answer to the given question. The context consists of excerpts from humanitarian documents. Your answer must be concise, precise, and based solely on the information available in the context. \n",
    "    #                 You should cite the appropriate contexts where necessary. Always cite for any factual claim. When citing several contexts, use [1][2][3]. Try to cite each sentence with at least one context.\n",
    "\n",
    "    # If the context does not contain a clear answer, respond with 'No clear answer.'\n",
    "\n",
    "    # Context:\n",
    "    # {context}\n",
    "\n",
    "    # Question:\n",
    "    # {question}\n",
    "\n",
    "    # Answer:\n",
    "    #                 ''' \n",
    "    \n",
    "            template = ''' \n",
    "            **Role:** You are an expert AI assistant specializing in answering questions *strictly* from provided source documents. Your primary directive is to provide accurate, concise answers, meticulously citing every piece of information.\n",
    "\n",
    "**Task:**\n",
    "1.  Carefully analyze the numbered source documents provided in the \"Context\" section.\n",
    "2.  Answer the user's \"Query\" *exclusively* using information found within these sources.\n",
    "3.  Every factual statement, phrase, or piece of data in your answer MUST be supported by a citation.\n",
    "\n",
    "**Instructions for Citation:**\n",
    "* **Granular Citation:** Place citations `[Source Number]` immediately after the *specific sentence or clause* that the information comes from. Aim for the smallest possible unit of text that can be attributed.\n",
    "* **Multiple Sources:** If a single piece of information is supported by content from multiple sources, list all relevant source numbers together without spaces, e.g., `[1][2][5]`.\n",
    "* **No Source, No Statement:** If you cannot find direct support for a piece of information in the provided sources, **DO NOT include that information** in your answer.\n",
    "* **Source Range:** Your citations *must* correspond to the provided source numbers (1, 2, 3... up to 10). Do not generate citation numbers outside this range.\n",
    "* **Prioritize Directness:** If multiple sources provide the same information, prioritize the most direct and clear phrasing, and cite all relevant sources.\n",
    "\n",
    "**Instructions for Answer Content:**\n",
    "* **Conciseness & Precision:** Keep your answer as concise and direct as possible, while fully addressing the query. Avoid conversational fillers or unnecessary elaboration.\n",
    "* **No Outside Knowledge:** Do NOT use any information, assumptions, or interpretations not explicitly stated in the provided sources. This is crucial for preventing hallucinations.\n",
    "* **Handling Missing Information:**\n",
    "    * If the provided sources contain *relevant* information but *do not definitively answer* the specific question, respond: `No clear answer.`\n",
    "    * If *none* of the sources contain *any* information relevant to the query, respond: `The provided sources do not contain information relevant to this query.`\n",
    "\n",
    "**Example:**\n",
    "Source 1: The sky is red in the evening and blue in the morning.\n",
    "Source 2: Water is wet when the sky is red.\n",
    "Source 3: Red skies often appear during sunset.\n",
    "\n",
    "Query: When is water wet?\n",
    "Answer: Water will be wet when the sky is red [2]. This phenomenon occurs in the evening [1], frequently during sunset [3].\n",
    "\n",
    "---\n",
    "\n",
    "**Context:**\n",
    "{context}\n",
    "\n",
    "**Query:** {question}\n",
    "\n",
    "**Answer:**\n",
    "            '''\n",
    "            prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "            # Create RAG chain using the retriever and prompt\n",
    "            \n",
    "            retrieved_context_for_prompt = (ragfusion_chain | extract_documents_from_scored_tuples | format_docs_for_prompt).invoke(question)\n",
    "            print(\"--- RETRIEVED CONTEXT PASSED TO THE MODEL ---\")\n",
    "            print(retrieved_context_for_prompt)\n",
    "            print(\"------------------------------------------\")\n",
    "            \n",
    "            full_rag_fusion_chain = (\n",
    "                {\n",
    "                    \"context\": lambda x: retrieved_context_for_prompt,\n",
    "                    \"question\": RunnablePassthrough()\n",
    "                }\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # Execute the RAG chain to get the answer\n",
    "            answer = full_rag_fusion_chain.invoke(question)\n",
    "            retrieved_docs = [doc.page_content for doc in retriever.get_relevant_documents(question)]\n",
    "            # scores = [doc.score for doc in retriever.get_relevant_documents(question)]\n",
    "            # rank = [doc.rank for doc in retriever.get_relevant_documents(question)]\n",
    "            print(\"Retrieved answer:\",  answer)\n",
    "            print(\"Real answer:\", trueanswer )\n",
    "            \n",
    "            # Append the results to the dictionary\n",
    "            data[\"question\"].append(question)\n",
    "            data[\"ground_truth_answer\"].append(trueanswer)\n",
    "            \n",
    "            data[\"retrieved_answer\"].append(answer)\n",
    "            data[\"retrieved_contexts\"].append(retrieved_docs)\n",
    "            data['cluster_idx'].append(idx)\n",
    "            # data['retrieved_scores'].append(scores)\n",
    "            # data['rank'].append(rank)\n",
    "\n",
    "#         em_answer = int(set(trueanswer) == set(answer))\n",
    "#         f1_answer = compute_f1_answers(trueanswer, answer)\n",
    "\n",
    "        \n",
    "        \n",
    "#         scores[i] = { \"em_answer\": em_answer, \"f1_answer\": f1_answer}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "\n",
    "# Iterate over the data and create a dictionary for each question\n",
    "for i in range(len(data[\"question\"])):\n",
    "    output_data.append({\n",
    "        'cluster_id': data[\"cluster_idx\"][i],\n",
    "        'question': data['question'][i],\n",
    "        #'ground_truth_answer': data['ground_truth_answer'][i],\n",
    "        'retrieved_answer': data['retrieved_answer'][i],\n",
    "        'retrieved_contexts': data['retrieved_contexts'][i],\n",
    "    })\n",
    "\n",
    "# Define the file path\n",
    "\n",
    "\n",
    "with open(answers_path_one_file, 'w') as f:\n",
    "    json.dump(output_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results rag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "for i in range(len(data[\"question\"])):\n",
    "    print(f\"Question: {data['question'][i]}\")\n",
    "    print(f\"Answer: {data['ground_truth_answer'][i]}\")\n",
    "    print(f\"Retrieved Answer: {data['retrieved_answer'][i]}\")\n",
    "    \n",
    "    # Format retrieved contexts for readability\n",
    "    print(\"Retrieved Contexts:\")\n",
    "    for context in data[\"retrieved_contexts\"][i]:\n",
    "        wrapped_context = textwrap.fill(context, width=80)  # Wrap text to 80 characters\n",
    "        print(f\"- {wrapped_context}\")\n",
    "    \n",
    "    print(\"-\" * 50 + \"\\n\")  # Separator and extra spacing between entries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
